import openai
import os
from typing import List, Optional
import logging
from src.data.database import Paper, DatabaseManager
from config.settings import settings
from pathlib import Path

logger = logging.getLogger(__name__)

class DeepSeekAnalyzer:
    def __init__(self, keyword: str = None):
        self.client = openai.OpenAI(
            api_key=settings.DEEPSEEK_API_KEY,
            base_url=settings.DEEPSEEK_BASE_URL
        )
        self.keyword = keyword
        # æ ¹æ®å…³é”®è¯è·å–æ•°æ®åº“ç®¡ç†å™¨
        if keyword:
            from src.data.keyword_manager import keyword_manager
            db_manager = keyword_manager.get_database_manager(keyword)
            self.db = db_manager
        else:
            # ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿æ•°æ®åº“è¿æ¥ä¸€è‡´æ€§
            db_path = settings.DATABASE_URL.replace("sqlite:///", "")
            if not os.path.isabs(db_path):
                # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºåŸºäºé¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
                project_root = Path(__file__).parent.parent.parent
                db_path = project_root / db_path
            self.db = DatabaseManager(str(db_path))
        # ç¡®ä¿insights_cacheè¡¨å­˜åœ¨
        self._init_insights_cache_table()

        # æ·»åŠ é”æœºåˆ¶é˜²æ­¢å¹¶å‘ç”Ÿæˆæ´å¯Ÿ
        self._generating_insights = set()  # å­˜å‚¨æ­£åœ¨ç”Ÿæˆçš„æ´å¯Ÿé”®

    def _init_insights_cache_table(self):
        """åˆå§‹åŒ–æ´å¯Ÿç¼“å­˜è¡¨"""
        try:
            import sqlite3
            with sqlite3.connect(self.db.db_path) as conn:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS insights_cache (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        cache_key TEXT UNIQUE NOT NULL,
                        data_hash TEXT NOT NULL,
                        insights TEXT NOT NULL,
                        trending TEXT NOT NULL,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                        updated_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                conn.commit()
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–insights_cacheè¡¨å¤±è´¥: {e}")

    def generate_summary(self, paper: Paper) -> Optional[str]:
        """
        ä½¿ç”¨DeepSeekç”Ÿæˆè®ºæ–‡æ‘˜è¦ - æ”¹è¿›ç‰ˆæœ¬ï¼Œç¡®ä¿ç¿»è¯‘å®Œæ•´æ€§

        Args:
            paper: è®ºæ–‡å¯¹è±¡

        Returns:
            ç”Ÿæˆçš„æ‘˜è¦æ–‡æœ¬
        """
        try:
            # æ ¹æ®æ‘˜è¦é•¿åº¦åŠ¨æ€è°ƒæ•´tokené™åˆ¶
            abstract_length = len(paper.abstract)
            if abstract_length < 500:
                max_tokens = 800  # çŸ­æ‘˜è¦ä½¿ç”¨è¾ƒå¤štoken
            elif abstract_length < 1000:
                max_tokens = 1200  # ä¸­ç­‰é•¿åº¦æ‘˜è¦
            else:
                max_tokens = 2000  # é•¿æ‘˜è¦ä½¿ç”¨æœ€å¤§tokené™åˆ¶

            # æ”¹è¿›çš„æç¤ºè¯ï¼Œæ˜ç¡®è¦æ±‚å®Œæ•´æ€§
            prompt = f"""
è¯·ä¸ºä»¥ä¸‹è®ºæ–‡ç”Ÿæˆä¸€ä¸ªå®Œæ•´ã€å‡†ç¡®çš„ä¸­æ–‡æ‘˜è¦ï¼Œç¡®ä¿è¦†ç›–åŸæ–‡æ‰€æœ‰å…³é”®ä¿¡æ¯ï¼š

æ ‡é¢˜ï¼š{paper.title}

ä½œè€…ï¼š{', '.join(paper.authors)}

è‹±æ–‡æ‘˜è¦ï¼š{paper.abstract}

åˆ†ç±»ï¼š{', '.join(paper.categories)}

è¦æ±‚ï¼š
1. å¿…é¡»å®Œæ•´ç¿»è¯‘å’Œæ¦‚æ‹¬åŸæ–‡æ‰€æœ‰é‡è¦ä¿¡æ¯ï¼Œä¸èƒ½é—æ¼å…³é”®å†…å®¹
2. çªå‡ºç ”ç©¶èƒŒæ™¯ã€ä¸»è¦è´¡çŒ®ã€æ–¹æ³•åˆ›æ–°ã€å®éªŒç»“æœå’Œç»“è®º
3. ä½¿ç”¨ä¸“ä¸šçš„å­¦æœ¯ä¸­æ–‡è¡¨è¾¾ï¼Œæœ¯è¯­å‡†ç¡®
4. æ ¹æ®åŸæ–‡é•¿åº¦ï¼Œä¸­æ–‡æ‘˜è¦åº”åœ¨300-800å­—ä¹‹é—´
5. ç¡®ä¿æŠ€æœ¯ç»†èŠ‚å’Œæ–¹æ³•æè¿°çš„å®Œæ•´æ€§
6. å¦‚æœåŸæ–‡å¾ˆé•¿ï¼Œè¯·é€‚å½“å¢åŠ æ‘˜è¦é•¿åº¦ä»¥ç¡®ä¿ä¿¡æ¯å®Œæ•´æ€§

è¯·æä¾›å®Œæ•´çš„ä¸­æ–‡æ‘˜è¦ï¼š
"""

            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡ç¿»è¯‘å’Œåˆ†æå¸ˆï¼Œæ“…é•¿å°†è‹±æ–‡ç§‘æŠ€è®ºæ–‡å‡†ç¡®ã€å®Œæ•´åœ°ç¿»è¯‘æˆä¸­æ–‡ï¼Œå¹¶ç¡®ä¿æ‰€æœ‰æŠ€æœ¯ç»†èŠ‚å’Œä¸“ä¸šæœ¯è¯­çš„æ­£ç¡®æ€§ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=0.5,  # ç¨å¾®æé«˜æ¸©åº¦ä»¥å¢åŠ è¡¨è¾¾çš„ä¸°å¯Œæ€§
                top_p=0.9,  # æ·»åŠ top_på‚æ•°æ§åˆ¶è¾“å‡ºçš„å¤šæ ·æ€§
                frequency_penalty=0.1,  # é¿å…é‡å¤
                presence_penalty=0.1  # é¼“åŠ±å¼•å…¥æ–°æ¦‚å¿µ
            )

            summary = response.choices[0].message.content.strip()

            # æ£€æŸ¥ç¿»è¯‘å®Œæ•´æ€§ - å¦‚æœæ‘˜è¦è¿‡çŸ­ï¼Œå°è¯•é‡æ–°ç”Ÿæˆ
            if len(summary) < 100 and abstract_length > 500:
                logger.warning(f"è®ºæ–‡ {paper.arxiv_id} çš„æ‘˜è¦å¯èƒ½ä¸å®Œæ•´ï¼Œå°è¯•é‡æ–°ç”Ÿæˆ")
                return self._regenerate_summary(paper, max_tokens)

            logger.info(f"æˆåŠŸç”Ÿæˆè®ºæ–‡ {paper.arxiv_id} çš„æ‘˜è¦ï¼Œé•¿åº¦ï¼š{len(summary)}å­—ç¬¦")
            return summary

        except Exception as e:
            logger.error(f"ç”Ÿæˆè®ºæ–‡æ‘˜è¦å¤±è´¥ {paper.arxiv_id}: {e}")
            return None

    def analyze_papers_batch(self, papers: List[Paper]) -> List[Paper]:
        """
        æ‰¹é‡åˆ†æè®ºæ–‡å¹¶ç”Ÿæˆæ‘˜è¦ - æ”¹è¿›ç‰ˆæœ¬ï¼Œç¡®ä¿ç¿»è¯‘è´¨é‡

        Args:
            papers: è®ºæ–‡åˆ—è¡¨

        Returns:
            å·²åˆ†æçš„è®ºæ–‡åˆ—è¡¨
        """
        analyzed_papers = []
        incomplete_count = 0
        retry_count = 0

        for i, paper in enumerate(papers):
            logger.info(f"åˆ†æè®ºæ–‡è¿›åº¦: {i+1}/{len(papers)} - {paper.title[:50]}...")

            # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ‘˜è¦
            if not paper.summary:
                summary = self.generate_summary(paper)
                if summary:
                    # æ£€æŸ¥æ‘˜è¦å®Œæ•´æ€§
                    if len(summary) < 150 and len(paper.abstract) > 600:
                        logger.warning(f"è®ºæ–‡ {paper.arxiv_id} æ‘˜è¦å¯èƒ½ä¸å®Œæ•´ï¼Œå°è¯•é‡æ–°ç”Ÿæˆ")
                        retry_summary = self._regenerate_summary(paper, 1500)
                        if retry_summary and len(retry_summary) > len(summary):
                            summary = retry_summary
                            retry_count += 1

                    paper.summary = summary
                    # æ›´æ–°æ•°æ®åº“
                    self._update_paper_summary(paper.arxiv_id, summary)

                    # è®°å½•æ‘˜è¦è´¨é‡ç»Ÿè®¡
                    summary_length = len(summary)
                    abstract_length = len(paper.abstract)
                    ratio = summary_length / abstract_length if abstract_length > 0 else 0

                    if ratio < 0.2 and abstract_length > 500:
                        logger.warning(f"è®ºæ–‡ {paper.arxiv_id} æ‘˜è¦æ¯”ä¾‹åä½ï¼š{ratio:.2f} ({summary_length}/{abstract_length})")
                        incomplete_count += 1
                    else:
                        logger.info(f"è®ºæ–‡ {paper.arxiv_id} æ‘˜è¦ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦æ¯”ä¾‹ï¼š{ratio:.2f}")
                else:
                    logger.error(f"è®ºæ–‡ {paper.arxiv_id} æ‘˜è¦ç”Ÿæˆå¤±è´¥")

            analyzed_papers.append(paper)

            # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶ - æ ¹æ®æ‘˜è¦é•¿åº¦è°ƒæ•´å»¶è¿Ÿ
            import time
            delay = min(2, 1 + len(paper.abstract) / 5000)  # é•¿æ‘˜è¦å¢åŠ å»¶è¿Ÿ
            time.sleep(delay)

        # æŠ¥å‘Šæ‰¹é‡åˆ†æç»“æœ
        total_analyzed = len([p for p in analyzed_papers if p.summary])
        logger.info(f"æ‰¹é‡åˆ†æå®Œæˆï¼šå…±{len(papers)}ç¯‡ï¼ŒæˆåŠŸ{total_analyzed}ç¯‡ï¼Œé‡æ–°ç”Ÿæˆ{retry_count}ç¯‡ï¼Œå¯èƒ½ä¸å®Œæ•´{incomplete_count}ç¯‡")

        return analyzed_papers

    def _update_paper_summary(self, arxiv_id: str, summary: str):
        """æ›´æ–°æ•°æ®åº“ä¸­çš„è®ºæ–‡æ‘˜è¦"""
        try:
            import sqlite3
            with sqlite3.connect(self.db.db_path) as conn:
                cursor = conn.execute(
                    "UPDATE papers SET summary = ? WHERE arxiv_id = ?",
                    (summary, arxiv_id)
                )
                conn.commit()

                # æ£€æŸ¥æ˜¯å¦çœŸæ­£æ›´æ–°äº†è®°å½•
                if cursor.rowcount > 0:
                    logger.info(f"æˆåŠŸæ›´æ–°è®ºæ–‡æ‘˜è¦: {arxiv_id}")
                    return True
                else:
                    logger.warning(f"æœªæ‰¾åˆ°è¦æ›´æ–°çš„è®ºæ–‡: {arxiv_id}")
                    return False

        except Exception as e:
            logger.error(f"æ›´æ–°è®ºæ–‡æ‘˜è¦å¤±è´¥ {arxiv_id}: {e}")
            return False

    def _regenerate_summary(self, paper: Paper, max_tokens: int) -> Optional[str]:
        """
        é‡æ–°ç”Ÿæˆæ‘˜è¦ - ç”¨äºå¤„ç†ä¸å®Œæ•´çš„ç¿»è¯‘

        Args:
            paper: è®ºæ–‡å¯¹è±¡
            max_tokens: tokené™åˆ¶

        Returns:
            é‡æ–°ç”Ÿæˆçš„æ‘˜è¦
        """
        try:
            # ä½¿ç”¨æ›´è¯¦ç»†çš„æç¤ºè¯é‡æ–°ç”Ÿæˆ
            prompt = f"""
ä»¥ä¸‹è®ºæ–‡çš„è‹±æ–‡æ‘˜è¦è¾ƒé•¿ï¼Œè¯·ç¡®ä¿å®Œæ•´ç¿»è¯‘å’Œæ¦‚æ‹¬æ‰€æœ‰é‡è¦ä¿¡æ¯ï¼š

æ ‡é¢˜ï¼š{paper.title}

è‹±æ–‡æ‘˜è¦ï¼š{paper.abstract}

è¦æ±‚ï¼š
1. å¿…é¡»å®Œæ•´è¦†ç›–åŸæ–‡æ‰€æœ‰å…³é”®ä¿¡æ¯ç‚¹
2. è¯¦ç»†æè¿°ç ”ç©¶èƒŒæ™¯ã€é—®é¢˜ã€æ–¹æ³•ã€å®éªŒå’Œç»“è®º
3. ä¸èƒ½é—æ¼ä»»ä½•é‡è¦çš„æŠ€æœ¯ç»†èŠ‚
4. ä½¿ç”¨å‡†ç¡®çš„ä¸“ä¸šæœ¯è¯­å’Œå­¦æœ¯è¡¨è¾¾
5. æ‘˜è¦é•¿åº¦åº”å……åˆ†åæ˜ åŸæ–‡å†…å®¹çš„ä¸°å¯Œç¨‹åº¦

è¯·æä¾›å®Œæ•´è¯¦ç»†çš„ä¸­æ–‡æ‘˜è¦ï¼š
"""

            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡ç¿»è¯‘ä¸“å®¶ï¼Œç¡®ä¿å°†è‹±æ–‡è®ºæ–‡å†…å®¹å®Œæ•´ã€å‡†ç¡®åœ°ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¸é—æ¼ä»»ä½•é‡è¦ä¿¡æ¯ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=0.7,  # æé«˜æ¸©åº¦ä»¥å¢åŠ è¯¦ç»†ç¨‹åº¦
                top_p=0.95
            )

            summary = response.choices[0].message.content.strip()
            logger.info(f"é‡æ–°ç”Ÿæˆè®ºæ–‡ {paper.arxiv_id} çš„æ‘˜è¦ï¼Œé•¿åº¦ï¼š{len(summary)}å­—ç¬¦")
            return summary

        except Exception as e:
            logger.error(f"é‡æ–°ç”Ÿæˆæ‘˜è¦å¤±è´¥ {paper.arxiv_id}: {e}")
            return None

    def get_research_insights(self, days: int = 7) -> str:
        """
        åŸºäºæœ€è¿‘çš„è®ºæ–‡ç”Ÿæˆç ”ç©¶æ´å¯Ÿï¼ˆæ™ºèƒ½ç¼“å­˜ç‰ˆæœ¬ï¼‰
        å¦‚æœæ•°æ®åº“æœªæ›´æ–°ï¼Œä½¿ç”¨æ°¸ä¹…ç¼“å­˜ï¼›å¦‚æœæ•°æ®åº“æ›´æ–°äº†ï¼Œé‡æ–°ç”Ÿæˆå¹¶ç¼“å­˜

        Args:
            days: åˆ†æçš„å¤©æ•°

        Returns:
            ç ”ç©¶æ´å¯Ÿæ–‡æœ¬
        """
        cache_key = f"insights_{days}"

        try:
            # æ£€æŸ¥æ˜¯å¦æ­£åœ¨ç”Ÿæˆæ´å¯Ÿï¼Œé¿å…é‡å¤ç”Ÿæˆ
            if cache_key in self._generating_insights:
                logger.info(f"æ´å¯Ÿ {cache_key} æ­£åœ¨ç”Ÿæˆä¸­ï¼Œè¿”å›ç°æœ‰ç¼“å­˜")
                cached_data = self.db.get_insights_cache(cache_key)
                if cached_data:
                    return cached_data['insights']
                else:
                    return "æ´å¯Ÿæ­£åœ¨ç”Ÿæˆä¸­ï¼Œè¯·ç¨ååˆ·æ–°..."

            # è·å–å½“å‰æ•°æ®çš„å“ˆå¸Œå€¼
            current_hash = self.db.get_data_hash(days)

            # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æ´å¯Ÿä¸”æ•°æ®æœªå˜åŒ–
            cached_data = self.db.get_insights_cache(cache_key)

            if cached_data and cached_data.get('data_hash') == current_hash:
                logger.info(f"æ•°æ®åº“æœªæ›´æ–°ï¼Œä½¿ç”¨æ°¸ä¹…ç¼“å­˜çš„æ´å¯Ÿæ•°æ®: {cache_key}")
                return cached_data['insights']

            logger.info(f"æ£€æµ‹åˆ°æ•°æ®åº“æ›´æ–°ï¼Œé‡æ–°ç”Ÿæˆæ´å¯Ÿ: {cache_key} (hash: {current_hash[:8]}...)")

            # æ·»åŠ åˆ°ç”Ÿæˆä¸­é›†åˆ
            self._generating_insights.add(cache_key)

            # æ•°æ®å˜åŒ–äº†ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆæ´å¯Ÿ
            papers = self.db.get_recent_papers(days)
            if not papers:
                return "æ²¡æœ‰æ‰¾åˆ°æœ€è¿‘çš„è®ºæ–‡æ•°æ®ã€‚"

            # å‡†å¤‡è®ºæ–‡ä¿¡æ¯ç”¨äºåˆ†æ
            papers_info = []
            # æ ¹æ®æ—¶é—´èŒƒå›´å’Œè®ºæ–‡æ•°é‡åŠ¨æ€è°ƒæ•´åˆ†ææ•°é‡
            total_papers = len(papers)

            # åŠ¨æ€ç¡®å®šè¦åˆ†æçš„è®ºæ–‡æ•°é‡
            if total_papers == 0:
                return "æ²¡æœ‰æ‰¾åˆ°æœ€è¿‘çš„è®ºæ–‡æ•°æ®ã€‚"
            elif total_papers <= 10:
                # è®ºæ–‡å¾ˆå°‘æ—¶ï¼Œåˆ†ææ‰€æœ‰è®ºæ–‡
                papers_to_analyze = papers
            elif total_papers <= 30:
                # è®ºæ–‡é€‚ä¸­æ—¶ï¼Œåˆ†æå¤§éƒ¨åˆ†è®ºæ–‡
                papers_to_analyze = papers[:min(25, total_papers)]
            else:
                # è®ºæ–‡å¾ˆå¤šæ—¶ï¼Œåˆ†æå›ºå®šæ•°é‡ä»¥ä¿è¯æ•ˆç‡å’Œè´¨é‡
                papers_to_analyze = papers[:30]  # å¢åŠ åˆ°30ç¯‡ä»¥è·å¾—æ›´å¥½çš„æ´å¯Ÿ
            for i, paper in enumerate(papers_to_analyze):
                papers_info.append({
                    'index': i + 1,
                    'title': paper.title,
                    'summary': paper.summary or paper.abstract[:400],  # ç¨å¾®ç¼©çŸ­æ‘˜è¦
                    'categories': paper.categories,
                    'authors': paper.authors[:3] if paper.authors else []  # åªå–å‰3ä¸ªä½œè€…
                })

            prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯åˆ†æå¸ˆã€‚è¯·åŸºäºæœ€è¿‘{days}å¤©çš„ç ”ç©¶è®ºæ–‡è¿›è¡Œåˆ†æï¼š

**æ•°æ®æ¦‚å†µï¼š**
- æ—¶é—´èŒƒå›´ï¼šæœ€è¿‘{days}å¤©
- æ€»è®ºæ–‡æ•°ï¼š{total_papers}ç¯‡
- æ·±åº¦åˆ†æï¼š{len(papers_to_analyze)}ç¯‡ï¼ˆ{len(papers_to_analyze)/total_papers*100:.1f}%ï¼‰

åˆ†æè®ºæ–‡ï¼š
{papers_info}

è¯·æä¾›æ·±å…¥çš„ç ”ç©¶æ´å¯Ÿåˆ†æï¼Œä½¿ç”¨ä¼˜ç¾çš„Markdownæ ¼å¼ï¼ŒåŒ…æ‹¬ï¼š

## ğŸ”¬ ç ”ç©¶è¶‹åŠ¿åˆ†æ
è¯†åˆ«å½“å‰æœ€ä¸»è¦çš„ç ”ç©¶çƒ­ç‚¹å’Œè¶‹åŠ¿æ–¹å‘

## âš¡ æŠ€æœ¯çªç ´ç‚¹
æ‰¾å‡ºé‡è¦çš„æŠ€æœ¯åˆ›æ–°å’Œæ–¹æ³•çªç ´

## ğŸ”— è·¨å­¦ç§‘èåˆ
è¯†åˆ«ä¸åŒç ”ç©¶é¢†åŸŸä¹‹é—´çš„äº¤å‰èåˆè¶‹åŠ¿

## ğŸ”® æœªæ¥å±•æœ›
åŸºäºå½“å‰ç ”ç©¶è¶‹åŠ¿é¢„æµ‹æœªæ¥å‘å±•æ–¹å‘

## ğŸ‘¥ å…³é”®ç ”ç©¶å›¢é˜Ÿ
è¯†åˆ«æ´»è·ƒçš„ç ”ç©¶æœºæ„å’Œä½œè€…ç¾¤ä½“

æ ¼å¼è¦æ±‚ï¼š
- ç”¨ä¸­æ–‡å›ç­”ï¼Œä½¿ç”¨ä¼˜é›…çš„Markdownæ ¼å¼
- æ¯ä¸ªä¸»è¦éƒ¨åˆ†ä½¿ç”¨åˆé€‚çš„emojiå›¾æ ‡
- é‡è¦æ¦‚å¿µå’Œå…³é”®è¯ä½¿ç”¨**åŠ ç²—**å¼ºè°ƒ
- æŠ€æœ¯æœ¯è¯­å’Œæ–¹æ³•ä½¿ç”¨`ä»£ç æ ¼å¼`æ ‡æ³¨
- é€‚å½“ä½¿ç”¨å¼•ç”¨å—>çªå‡ºé‡ç‚¹è§‚ç‚¹
- ç¡®ä¿å†…å®¹ç»“æ„æ¸…æ™°ï¼Œå±‚æ¬¡åˆ†æ˜
- å­—æ•°æ§åˆ¶åœ¨800-1200å­—ä¹‹é—´
- åŸºäºå®é™…è®ºæ–‡å†…å®¹è¿›è¡Œåˆ†æï¼Œä¸è¦æ³›æ³›è€Œè°ˆ
"""

            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶è¶‹åŠ¿åˆ†æå¸ˆï¼Œæ“…é•¿ä»å­¦æœ¯è®ºæ–‡ä¸­æå–æœ‰ä»·å€¼çš„æ´å¯Ÿã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=3000,  # å¢åŠ tokené™åˆ¶ä»¥æ”¯æŒæ›´é•¿çš„åˆ†æ
                temperature=0.3,   # é™ä½éšæœºæ€§ï¼Œæé«˜ç¨³å®šæ€§
            )

            insights = response.choices[0].message.content.strip()
            logger.info("æˆåŠŸç”Ÿæˆç ”ç©¶æ´å¯Ÿ")

            # åŒæ—¶è·å–çƒ­é—¨ä¸»é¢˜å¹¶ä¿å­˜åˆ°ç¼“å­˜
            try:
                trending_topics = self.get_trending_topics(days)

                # ä¿å­˜åˆ°æ•°æ®åº“ç¼“å­˜
                self.db.save_insights_cache(cache_key, current_hash, insights, trending_topics)
                logger.info(f"æ´å¯Ÿå·²ç¼“å­˜åˆ°æ•°æ®åº“: {cache_key}")

            except Exception as e:
                logger.warning(f"è·å–çƒ­é—¨ä¸»é¢˜å¤±è´¥: {e}")
                # ä¿å­˜åˆ°æ•°æ®åº“ç¼“å­˜ï¼ˆä¸åŒ…å«çƒ­é—¨ä¸»é¢˜ï¼‰
                try:
                    self.db.save_insights_cache(cache_key, current_hash, insights, [])
                except Exception as cache_e:
                    logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {cache_e}")

            return insights

        except Exception as e:
            logger.error(f"ç”Ÿæˆç ”ç©¶æ´å¯Ÿå¤±è´¥: {e}")
            return f"ç”Ÿæˆæ´å¯Ÿå¤±è´¥: {str(e)}"
        finally:
            # ç¡®ä¿ä»ç”Ÿæˆä¸­é›†åˆä¸­ç§»é™¤ï¼Œå³ä½¿å‡ºç°å¼‚å¸¸
            if cache_key in self._generating_insights:
                self._generating_insights.remove(cache_key)

    def auto_update_insights_if_needed(self, days: int = 7) -> bool:
        """
        æ£€æŸ¥æ•°æ®åº“æ˜¯å¦æ›´æ–°ï¼Œå¦‚æœæ›´æ–°äº†åˆ™è‡ªåŠ¨ç”Ÿæˆæ–°çš„æ´å¯Ÿï¼ˆåå°ä»»åŠ¡ç”¨ï¼‰

        Args:
            days: åˆ†æçš„å¤©æ•°

        Returns:
            æ˜¯å¦æ›´æ–°äº†æ´å¯Ÿ
        """
        cache_key = f"insights_{days}"

        try:
            # è·å–å½“å‰æ•°æ®çš„å“ˆå¸Œå€¼
            current_hash = self.db.get_data_hash(days)

            # æ£€æŸ¥ç¼“å­˜çš„æ•°æ®å“ˆå¸Œ
            cached_data = self.db.get_insights_cache(cache_key)

            if not cached_data or cached_data.get('data_hash') != current_hash:
                logger.info(f"æ£€æµ‹åˆ°æ•°æ®åº“æ›´æ–°ï¼Œåå°è‡ªåŠ¨ç”Ÿæˆæ´å¯Ÿ: {cache_key}")

                # å¼‚æ­¥ç”Ÿæˆæ´å¯Ÿ
                insights = self.get_research_insights(days)

                if insights and not insights.startswith("ç”Ÿæˆæ´å¯Ÿå¤±è´¥"):
                    logger.info(f"åå°æ´å¯Ÿç”ŸæˆæˆåŠŸ: {cache_key}")
                    return True
                else:
                    logger.error(f"åå°æ´å¯Ÿç”Ÿæˆå¤±è´¥: {cache_key}")
                    return False
            else:
                logger.debug(f"æ•°æ®åº“æœªæ›´æ–°ï¼Œè·³è¿‡æ´å¯Ÿç”Ÿæˆ: {cache_key}")
                return False

        except Exception as e:
            logger.error(f"è‡ªåŠ¨æ›´æ–°æ´å¯Ÿæ£€æŸ¥å¤±è´¥: {e}")
            return False

    def get_trending_topics(self, days: int = 7) -> List[str]:
        """
        è·å–çƒ­é—¨ç ”ç©¶ä¸»é¢˜
        åŸºäºæœ€è¿‘è®ºæ–‡çš„æ ‡é¢˜å’Œæ‘˜è¦åˆ†æå‡ºé«˜é¢‘å…³é”®è¯

        Args:
            days: åˆ†æçš„å¤©æ•°

        Returns:
            çƒ­é—¨ä¸»é¢˜åˆ—è¡¨
        """
        try:
            papers = self.db.get_recent_papers(days)
            if not papers:
                return []

            # æ”¶é›†æ‰€æœ‰æ ‡é¢˜å’Œæ‘˜è¦ä¸­çš„å…³é”®è¯
            all_text = []
            for paper in papers:
                # ä½¿ç”¨æ ‡é¢˜å’Œæ‘˜è¦ï¼ˆå¦‚æœæœ‰ç”Ÿæˆæ‘˜è¦åˆ™ç”¨æ‘˜è¦ï¼Œå¦åˆ™ç”¨åŸæ‘˜è¦ï¼‰
                text = paper.title.lower()
                summary_text = paper.summary if paper.summary else paper.abstract
                text += " " + summary_text.lower()
                all_text.append(text)

            # å¸¸è§çš„æŠ€æœ¯è¯æ±‡è¿‡æ»¤
            common_words = {
                'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
                'by', 'from', 'we', 'our', 'this', 'that', 'these', 'those', 'is', 'are', 'was',
                'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
                'can', 'could', 'may', 'might', 'should', 'shall', 'must', 'paper', 'study',
                'research', 'analysis', 'approach', 'method', 'model', 'system', 'algorithm',
                'propose', 'present', 'show', 'demonstrate', 'evaluate', 'performance', 'result'
            }

            # ç®€å•çš„å…³é”®è¯æå–
            import re
            word_count = {}

            for text in all_text:
                # æå–å•è¯ï¼ˆä¿ç•™æŠ€æœ¯æœ¯è¯­ï¼‰
                words = re.findall(r'\b[a-zA-Z-]{3,}\b', text)
                for word in words:
                    if word not in common_words and len(word) > 3:
                        word_count[word] = word_count.get(word, 0) + 1

            # è·å–å‡ºç°é¢‘ç‡æœ€é«˜çš„å…³é”®è¯
            sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)

            # è¿”å›å‰10ä¸ªçƒ­é—¨å…³é”®è¯
            trending_topics = [word for word, count in sorted_words[:10] if count >= 2]

            logger.info(f"æå–åˆ°{len(trending_topics)}ä¸ªçƒ­é—¨ä¸»é¢˜")
            return trending_topics

        except Exception as e:
            logger.error(f"è·å–çƒ­é—¨ä¸»é¢˜å¤±è´¥: {e}")
            return []

    def compare_papers(self, paper_ids: List[str]) -> str:
        """
        æ¯”è¾ƒå¤šç¯‡è®ºæ–‡çš„å¼‚åŒ

        Args:
            paper_ids: è®ºæ–‡arXiv IDåˆ—è¡¨

        Returns:
            æ¯”è¾ƒåˆ†æç»“æœ
        """
        try:
            papers = []
            found_ids = []
            missing_ids = []

            for paper_id in paper_ids:
                # ä»æ•°æ®åº“è·å–è®ºæ–‡
                import sqlite3
                with sqlite3.connect(self.db.db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute(
                        "SELECT title, abstract, summary FROM papers WHERE arxiv_id = ?",
                        (paper_id,)
                    )
                    result = cursor.fetchone()
                    if result:
                        papers.append({
                            'id': paper_id,
                            'title': result[0],
                            'abstract': result[1],
                            'summary': result[2] or result[1][:500]
                        })
                        found_ids.append(paper_id)
                    else:
                        missing_ids.append(paper_id)

            # å¦‚æœæœ‰ç¼ºå¤±çš„è®ºæ–‡ï¼Œæä¾›è¯¦ç»†é”™è¯¯ä¿¡æ¯
            if missing_ids:
                error_msg = f"ä»¥ä¸‹è®ºæ–‡åœ¨æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ï¼š{', '.join(missing_ids)}\n\n"
                if found_ids:
                    error_msg += f"æ‰¾åˆ°çš„è®ºæ–‡ï¼š{', '.join(found_ids)}"
                else:
                    error_msg += "è¯·æ£€æŸ¥è®ºæ–‡IDæ˜¯å¦æ­£ç¡®ï¼Œæˆ–å…ˆçˆ¬å–è¿™äº›è®ºæ–‡ã€‚\n\næç¤ºï¼š\n"
                    error_msg += "1. è®¿é—®è®ºæ–‡åˆ—è¡¨é¡µé¢æŸ¥çœ‹å¯ç”¨çš„è®ºæ–‡ID\n"
                    error_msg += "2. ä½¿ç”¨çˆ¬è™«åŠŸèƒ½è·å–æ–°è®ºæ–‡\n"
                    error_msg += "3. ç¡®ä¿è®ºæ–‡IDæ ¼å¼æ­£ç¡®ï¼ˆå¦‚ï¼š2510.25767ï¼‰"
                return error_msg

            if len(papers) < 2:
                return "éœ€è¦è‡³å°‘ä¸¤ç¯‡è®ºæ–‡è¿›è¡Œæ¯”è¾ƒã€‚"

            prompt = f"""
è¯·æ¯”è¾ƒåˆ†æä»¥ä¸‹å‡ ç¯‡è®ºæ–‡çš„å¼‚åŒç‚¹ï¼š

{papers}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œæ¯”è¾ƒï¼š
1. ç ”ç©¶ç›®æ ‡å’Œé—®é¢˜çš„å¼‚åŒ
2. æ–¹æ³•è®ºçš„åŒºåˆ«
3. ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹
4. ä¼˜ç¼ºç‚¹å¯¹æ¯”
5. é€‚ç”¨åœºæ™¯çš„å·®å¼‚

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œç»“æ„åŒ–è¾“å‡ºã€‚
"""

            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯æ¯”è¾ƒåˆ†æå¸ˆï¼Œæ“…é•¿æ·±å…¥åˆ†æå’Œæ¯”è¾ƒä¸åŒç ”ç©¶è®ºæ–‡ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.3
            )

            comparison = response.choices[0].message.content.strip()
            logger.info(f"æˆåŠŸå®Œæˆ {len(papers)} ç¯‡è®ºæ–‡çš„æ¯”è¾ƒåˆ†æ")
            return comparison

        except Exception as e:
            logger.error(f"è®ºæ–‡æ¯”è¾ƒåˆ†æå¤±è´¥: {e}")
            return "è®ºæ–‡æ¯”è¾ƒåˆ†ææ—¶å‡ºç°é”™è¯¯ã€‚"