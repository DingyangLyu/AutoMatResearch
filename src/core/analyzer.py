import openai
import os
from typing import List, Optional
import logging
from src.data.database import Paper, DatabaseManager
from config.settings import settings
from pathlib import Path

logger = logging.getLogger(__name__)

class DeepSeekAnalyzer:
    def __init__(self):
        self.client = openai.OpenAI(
            api_key=settings.DEEPSEEK_API_KEY,
            base_url=settings.DEEPSEEK_BASE_URL
        )
        # ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿æ•°æ®åº“è¿æ¥ä¸€è‡´æ€§
        db_path = settings.DATABASE_URL.replace("sqlite:///", "")
        if not os.path.isabs(db_path):
            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºåŸºäºé¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
            project_root = Path(__file__).parent.parent.parent
            db_path = project_root / db_path
        self.db = DatabaseManager(str(db_path))
        # ç¡®ä¿insights_cacheè¡¨å­˜åœ¨
        self._init_insights_cache_table()

        # æ·»åŠ é”æœºåˆ¶é˜²æ­¢å¹¶å‘ç”Ÿæˆæ´å¯Ÿ
        self._generating_insights = set()  # å­˜å‚¨æ­£åœ¨ç”Ÿæˆçš„æ´å¯Ÿé”®

    def _init_insights_cache_table(self):
        """åˆå§‹åŒ–æ´å¯Ÿç¼“å­˜è¡¨"""
        try:
            import sqlite3
            with sqlite3.connect(self.db.db_path) as conn:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS insights_cache (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        cache_key TEXT UNIQUE NOT NULL,
                        data_hash TEXT NOT NULL,
                        insights TEXT NOT NULL,
                        trending TEXT NOT NULL,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                        updated_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                conn.commit()
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–insights_cacheè¡¨å¤±è´¥: {e}")

    def generate_summary(self, paper: Paper) -> Optional[str]:
        """
        ä½¿ç”¨DeepSeekç”Ÿæˆè®ºæ–‡æ‘˜è¦

        Args:
            paper: è®ºæ–‡å¯¹è±¡

        Returns:
            ç”Ÿæˆçš„æ‘˜è¦æ–‡æœ¬
        """
        try:
            prompt = f"""
è¯·ä¸ºä»¥ä¸‹è®ºæ–‡ç”Ÿæˆä¸€ä¸ªç®€æ´çš„ä¸­æ–‡æ‘˜è¦ï¼Œçªå‡ºå…¶ä¸»è¦è´¡çŒ®ã€æ–¹æ³•å’Œç»“è®ºï¼š

æ ‡é¢˜ï¼š{paper.title}

ä½œè€…ï¼š{', '.join(paper.authors)}

æ‘˜è¦ï¼š{paper.abstract}

åˆ†ç±»ï¼š{', '.join(paper.categories)}

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œæ§åˆ¶åœ¨200-300å­—ä»¥å†…ã€‚
"""

            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æå¸ˆï¼Œæ“…é•¿æå–è®ºæ–‡çš„æ ¸å¿ƒå†…å®¹å’Œè´¡çŒ®ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.3
            )

            summary = response.choices[0].message.content.strip()
            logger.info(f"æˆåŠŸç”Ÿæˆè®ºæ–‡ {paper.arxiv_id} çš„æ‘˜è¦")
            return summary

        except Exception as e:
            logger.error(f"ç”Ÿæˆè®ºæ–‡æ‘˜è¦å¤±è´¥ {paper.arxiv_id}: {e}")
            return None

    def analyze_papers_batch(self, papers: List[Paper]) -> List[Paper]:
        """
        æ‰¹é‡åˆ†æè®ºæ–‡å¹¶ç”Ÿæˆæ‘˜è¦

        Args:
            papers: è®ºæ–‡åˆ—è¡¨

        Returns:
            å·²åˆ†æçš„è®ºæ–‡åˆ—è¡¨
        """
        analyzed_papers = []

        for i, paper in enumerate(papers):
            logger.info(f"åˆ†æè®ºæ–‡è¿›åº¦: {i+1}/{len(papers)} - {paper.title[:50]}...")

            # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ‘˜è¦
            if not paper.summary:
                summary = self.generate_summary(paper)
                if summary:
                    paper.summary = summary
                    # æ›´æ–°æ•°æ®åº“
                    self._update_paper_summary(paper.arxiv_id, summary)

            analyzed_papers.append(paper)

            # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶
            import time
            time.sleep(1)

        return analyzed_papers

    def _update_paper_summary(self, arxiv_id: str, summary: str):
        """æ›´æ–°æ•°æ®åº“ä¸­çš„è®ºæ–‡æ‘˜è¦"""
        try:
            import sqlite3
            with sqlite3.connect(self.db.db_path) as conn:
                conn.execute(
                    "UPDATE papers SET summary = ? WHERE arxiv_id = ?",
                    (summary, arxiv_id)
                )
                conn.commit()
        except Exception as e:
            logger.error(f"æ›´æ–°è®ºæ–‡æ‘˜è¦å¤±è´¥ {arxiv_id}: {e}")

    def get_research_insights(self, days: int = 7) -> str:
        """
        åŸºäºæœ€è¿‘çš„è®ºæ–‡ç”Ÿæˆç ”ç©¶æ´å¯Ÿï¼ˆæ™ºèƒ½ç¼“å­˜ç‰ˆæœ¬ï¼‰
        å¦‚æœæ•°æ®åº“æœªæ›´æ–°ï¼Œä½¿ç”¨æ°¸ä¹…ç¼“å­˜ï¼›å¦‚æœæ•°æ®åº“æ›´æ–°äº†ï¼Œé‡æ–°ç”Ÿæˆå¹¶ç¼“å­˜

        Args:
            days: åˆ†æçš„å¤©æ•°

        Returns:
            ç ”ç©¶æ´å¯Ÿæ–‡æœ¬
        """
        cache_key = f"insights_{days}"

        try:
            # æ£€æŸ¥æ˜¯å¦æ­£åœ¨ç”Ÿæˆæ´å¯Ÿï¼Œé¿å…é‡å¤ç”Ÿæˆ
            if cache_key in self._generating_insights:
                logger.info(f"æ´å¯Ÿ {cache_key} æ­£åœ¨ç”Ÿæˆä¸­ï¼Œè¿”å›ç°æœ‰ç¼“å­˜")
                cached_data = self.db.get_insights_cache(cache_key)
                if cached_data:
                    return cached_data['insights']
                else:
                    return "æ´å¯Ÿæ­£åœ¨ç”Ÿæˆä¸­ï¼Œè¯·ç¨ååˆ·æ–°..."

            # è·å–å½“å‰æ•°æ®çš„å“ˆå¸Œå€¼
            current_hash = self.db.get_data_hash(days)

            # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æ´å¯Ÿä¸”æ•°æ®æœªå˜åŒ–
            cached_data = self.db.get_insights_cache(cache_key)

            if cached_data and cached_data.get('data_hash') == current_hash:
                logger.info(f"æ•°æ®åº“æœªæ›´æ–°ï¼Œä½¿ç”¨æ°¸ä¹…ç¼“å­˜çš„æ´å¯Ÿæ•°æ®: {cache_key}")
                return cached_data['insights']

            logger.info(f"æ£€æµ‹åˆ°æ•°æ®åº“æ›´æ–°ï¼Œé‡æ–°ç”Ÿæˆæ´å¯Ÿ: {cache_key} (hash: {current_hash[:8]}...)")

            # æ·»åŠ åˆ°ç”Ÿæˆä¸­é›†åˆ
            self._generating_insights.add(cache_key)

            # æ•°æ®å˜åŒ–äº†ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆæ´å¯Ÿ
            papers = self.db.get_recent_papers(days)
            if not papers:
                return "æ²¡æœ‰æ‰¾åˆ°æœ€è¿‘çš„è®ºæ–‡æ•°æ®ã€‚"

            # å‡†å¤‡è®ºæ–‡ä¿¡æ¯ç”¨äºåˆ†æ
            papers_info = []
            # æ ¹æ®æ—¶é—´èŒƒå›´å’Œè®ºæ–‡æ•°é‡åŠ¨æ€è°ƒæ•´åˆ†ææ•°é‡
            total_papers = len(papers)

            # åŠ¨æ€ç¡®å®šè¦åˆ†æçš„è®ºæ–‡æ•°é‡
            if total_papers == 0:
                return "æ²¡æœ‰æ‰¾åˆ°æœ€è¿‘çš„è®ºæ–‡æ•°æ®ã€‚"
            elif total_papers <= 10:
                # è®ºæ–‡å¾ˆå°‘æ—¶ï¼Œåˆ†ææ‰€æœ‰è®ºæ–‡
                papers_to_analyze = papers
            elif total_papers <= 30:
                # è®ºæ–‡é€‚ä¸­æ—¶ï¼Œåˆ†æå¤§éƒ¨åˆ†è®ºæ–‡
                papers_to_analyze = papers[:min(25, total_papers)]
            else:
                # è®ºæ–‡å¾ˆå¤šæ—¶ï¼Œåˆ†æå›ºå®šæ•°é‡ä»¥ä¿è¯æ•ˆç‡å’Œè´¨é‡
                papers_to_analyze = papers[:30]  # å¢åŠ åˆ°30ç¯‡ä»¥è·å¾—æ›´å¥½çš„æ´å¯Ÿ
            for i, paper in enumerate(papers_to_analyze):
                papers_info.append({
                    'index': i + 1,
                    'title': paper.title,
                    'summary': paper.summary or paper.abstract[:400],  # ç¨å¾®ç¼©çŸ­æ‘˜è¦
                    'categories': paper.categories,
                    'authors': paper.authors[:3] if paper.authors else []  # åªå–å‰3ä¸ªä½œè€…
                })

            prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯åˆ†æå¸ˆã€‚è¯·åŸºäºæœ€è¿‘{days}å¤©çš„ç ”ç©¶è®ºæ–‡è¿›è¡Œåˆ†æï¼š

**æ•°æ®æ¦‚å†µï¼š**
- æ—¶é—´èŒƒå›´ï¼šæœ€è¿‘{days}å¤©
- æ€»è®ºæ–‡æ•°ï¼š{total_papers}ç¯‡
- æ·±åº¦åˆ†æï¼š{len(papers_to_analyze)}ç¯‡ï¼ˆ{len(papers_to_analyze)/total_papers*100:.1f}%ï¼‰

åˆ†æè®ºæ–‡ï¼š
{papers_info}

è¯·æä¾›æ·±å…¥çš„ç ”ç©¶æ´å¯Ÿåˆ†æï¼Œä½¿ç”¨ä¼˜ç¾çš„Markdownæ ¼å¼ï¼ŒåŒ…æ‹¬ï¼š

## ğŸ”¬ ç ”ç©¶è¶‹åŠ¿åˆ†æ
è¯†åˆ«å½“å‰æœ€ä¸»è¦çš„ç ”ç©¶çƒ­ç‚¹å’Œè¶‹åŠ¿æ–¹å‘

## âš¡ æŠ€æœ¯çªç ´ç‚¹
æ‰¾å‡ºé‡è¦çš„æŠ€æœ¯åˆ›æ–°å’Œæ–¹æ³•çªç ´

## ğŸ”— è·¨å­¦ç§‘èåˆ
è¯†åˆ«ä¸åŒç ”ç©¶é¢†åŸŸä¹‹é—´çš„äº¤å‰èåˆè¶‹åŠ¿

## ğŸ”® æœªæ¥å±•æœ›
åŸºäºå½“å‰ç ”ç©¶è¶‹åŠ¿é¢„æµ‹æœªæ¥å‘å±•æ–¹å‘

## ğŸ‘¥ å…³é”®ç ”ç©¶å›¢é˜Ÿ
è¯†åˆ«æ´»è·ƒçš„ç ”ç©¶æœºæ„å’Œä½œè€…ç¾¤ä½“

æ ¼å¼è¦æ±‚ï¼š
- ç”¨ä¸­æ–‡å›ç­”ï¼Œä½¿ç”¨ä¼˜é›…çš„Markdownæ ¼å¼
- æ¯ä¸ªä¸»è¦éƒ¨åˆ†ä½¿ç”¨åˆé€‚çš„emojiå›¾æ ‡
- é‡è¦æ¦‚å¿µå’Œå…³é”®è¯ä½¿ç”¨**åŠ ç²—**å¼ºè°ƒ
- æŠ€æœ¯æœ¯è¯­å’Œæ–¹æ³•ä½¿ç”¨`ä»£ç æ ¼å¼`æ ‡æ³¨
- é€‚å½“ä½¿ç”¨å¼•ç”¨å—>çªå‡ºé‡ç‚¹è§‚ç‚¹
- ç¡®ä¿å†…å®¹ç»“æ„æ¸…æ™°ï¼Œå±‚æ¬¡åˆ†æ˜
- å­—æ•°æ§åˆ¶åœ¨800-1200å­—ä¹‹é—´
- åŸºäºå®é™…è®ºæ–‡å†…å®¹è¿›è¡Œåˆ†æï¼Œä¸è¦æ³›æ³›è€Œè°ˆ
"""

            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶è¶‹åŠ¿åˆ†æå¸ˆï¼Œæ“…é•¿ä»å­¦æœ¯è®ºæ–‡ä¸­æå–æœ‰ä»·å€¼çš„æ´å¯Ÿã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=3000,  # å¢åŠ tokené™åˆ¶ä»¥æ”¯æŒæ›´é•¿çš„åˆ†æ
                temperature=0.3,   # é™ä½éšæœºæ€§ï¼Œæé«˜ç¨³å®šæ€§
            )

            insights = response.choices[0].message.content.strip()
            logger.info("æˆåŠŸç”Ÿæˆç ”ç©¶æ´å¯Ÿ")

            # åŒæ—¶è·å–çƒ­é—¨ä¸»é¢˜å¹¶ä¿å­˜åˆ°ç¼“å­˜
            try:
                trending_topics = self.get_trending_topics(days)

                # ä¿å­˜åˆ°æ•°æ®åº“ç¼“å­˜
                self.db.save_insights_cache(cache_key, current_hash, insights, trending_topics)
                logger.info(f"æ´å¯Ÿå·²ç¼“å­˜åˆ°æ•°æ®åº“: {cache_key}")

            except Exception as e:
                logger.warning(f"è·å–çƒ­é—¨ä¸»é¢˜å¤±è´¥: {e}")
                # ä¿å­˜åˆ°æ•°æ®åº“ç¼“å­˜ï¼ˆä¸åŒ…å«çƒ­é—¨ä¸»é¢˜ï¼‰
                try:
                    self.db.save_insights_cache(cache_key, current_hash, insights, [])
                except Exception as cache_e:
                    logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {cache_e}")

            return insights

        except Exception as e:
            logger.error(f"ç”Ÿæˆç ”ç©¶æ´å¯Ÿå¤±è´¥: {e}")
            return f"ç”Ÿæˆæ´å¯Ÿå¤±è´¥: {str(e)}"
        finally:
            # ç¡®ä¿ä»ç”Ÿæˆä¸­é›†åˆä¸­ç§»é™¤ï¼Œå³ä½¿å‡ºç°å¼‚å¸¸
            if cache_key in self._generating_insights:
                self._generating_insights.remove(cache_key)

    def auto_update_insights_if_needed(self, days: int = 7) -> bool:
        """
        æ£€æŸ¥æ•°æ®åº“æ˜¯å¦æ›´æ–°ï¼Œå¦‚æœæ›´æ–°äº†åˆ™è‡ªåŠ¨ç”Ÿæˆæ–°çš„æ´å¯Ÿï¼ˆåå°ä»»åŠ¡ç”¨ï¼‰

        Args:
            days: åˆ†æçš„å¤©æ•°

        Returns:
            æ˜¯å¦æ›´æ–°äº†æ´å¯Ÿ
        """
        cache_key = f"insights_{days}"

        try:
            # è·å–å½“å‰æ•°æ®çš„å“ˆå¸Œå€¼
            current_hash = self.db.get_data_hash(days)

            # æ£€æŸ¥ç¼“å­˜çš„æ•°æ®å“ˆå¸Œ
            cached_data = self.db.get_insights_cache(cache_key)

            if not cached_data or cached_data.get('data_hash') != current_hash:
                logger.info(f"æ£€æµ‹åˆ°æ•°æ®åº“æ›´æ–°ï¼Œåå°è‡ªåŠ¨ç”Ÿæˆæ´å¯Ÿ: {cache_key}")

                # å¼‚æ­¥ç”Ÿæˆæ´å¯Ÿ
                insights = self.get_research_insights(days)

                if insights and not insights.startswith("ç”Ÿæˆæ´å¯Ÿå¤±è´¥"):
                    logger.info(f"åå°æ´å¯Ÿç”ŸæˆæˆåŠŸ: {cache_key}")
                    return True
                else:
                    logger.error(f"åå°æ´å¯Ÿç”Ÿæˆå¤±è´¥: {cache_key}")
                    return False
            else:
                logger.debug(f"æ•°æ®åº“æœªæ›´æ–°ï¼Œè·³è¿‡æ´å¯Ÿç”Ÿæˆ: {cache_key}")
                return False

        except Exception as e:
            logger.error(f"è‡ªåŠ¨æ›´æ–°æ´å¯Ÿæ£€æŸ¥å¤±è´¥: {e}")
            return False

    def get_trending_topics(self, days: int = 7) -> List[str]:
        """
        è·å–çƒ­é—¨ç ”ç©¶ä¸»é¢˜
        åŸºäºæœ€è¿‘è®ºæ–‡çš„æ ‡é¢˜å’Œæ‘˜è¦åˆ†æå‡ºé«˜é¢‘å…³é”®è¯

        Args:
            days: åˆ†æçš„å¤©æ•°

        Returns:
            çƒ­é—¨ä¸»é¢˜åˆ—è¡¨
        """
        try:
            papers = self.db.get_recent_papers(days)
            if not papers:
                return []

            # æ”¶é›†æ‰€æœ‰æ ‡é¢˜å’Œæ‘˜è¦ä¸­çš„å…³é”®è¯
            all_text = []
            for paper in papers:
                # ä½¿ç”¨æ ‡é¢˜å’Œæ‘˜è¦ï¼ˆå¦‚æœæœ‰ç”Ÿæˆæ‘˜è¦åˆ™ç”¨æ‘˜è¦ï¼Œå¦åˆ™ç”¨åŸæ‘˜è¦ï¼‰
                text = paper.title.lower()
                summary_text = paper.summary if paper.summary else paper.abstract
                text += " " + summary_text.lower()
                all_text.append(text)

            # å¸¸è§çš„æŠ€æœ¯è¯æ±‡è¿‡æ»¤
            common_words = {
                'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
                'by', 'from', 'we', 'our', 'this', 'that', 'these', 'those', 'is', 'are', 'was',
                'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
                'can', 'could', 'may', 'might', 'should', 'shall', 'must', 'paper', 'study',
                'research', 'analysis', 'approach', 'method', 'model', 'system', 'algorithm',
                'propose', 'present', 'show', 'demonstrate', 'evaluate', 'performance', 'result'
            }

            # ç®€å•çš„å…³é”®è¯æå–
            import re
            word_count = {}

            for text in all_text:
                # æå–å•è¯ï¼ˆä¿ç•™æŠ€æœ¯æœ¯è¯­ï¼‰
                words = re.findall(r'\b[a-zA-Z-]{3,}\b', text)
                for word in words:
                    if word not in common_words and len(word) > 3:
                        word_count[word] = word_count.get(word, 0) + 1

            # è·å–å‡ºç°é¢‘ç‡æœ€é«˜çš„å…³é”®è¯
            sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)

            # è¿”å›å‰10ä¸ªçƒ­é—¨å…³é”®è¯
            trending_topics = [word for word, count in sorted_words[:10] if count >= 2]

            logger.info(f"æå–åˆ°{len(trending_topics)}ä¸ªçƒ­é—¨ä¸»é¢˜")
            return trending_topics

        except Exception as e:
            logger.error(f"è·å–çƒ­é—¨ä¸»é¢˜å¤±è´¥: {e}")
            return []

    def compare_papers(self, paper_ids: List[str]) -> str:
        """
        æ¯”è¾ƒå¤šç¯‡è®ºæ–‡çš„å¼‚åŒ

        Args:
            paper_ids: è®ºæ–‡arXiv IDåˆ—è¡¨

        Returns:
            æ¯”è¾ƒåˆ†æç»“æœ
        """
        try:
            papers = []
            for paper_id in paper_ids:
                # ä»æ•°æ®åº“è·å–è®ºæ–‡
                import sqlite3
                with sqlite3.connect(self.db.db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute(
                        "SELECT title, abstract, summary FROM papers WHERE arxiv_id = ?",
                        (paper_id,)
                    )
                    result = cursor.fetchone()
                    if result:
                        papers.append({
                            'id': paper_id,
                            'title': result[0],
                            'abstract': result[1],
                            'summary': result[2] or result[1][:500]
                        })

            if len(papers) < 2:
                return "éœ€è¦è‡³å°‘ä¸¤ç¯‡è®ºæ–‡è¿›è¡Œæ¯”è¾ƒã€‚"

            prompt = f"""
è¯·æ¯”è¾ƒåˆ†æä»¥ä¸‹å‡ ç¯‡è®ºæ–‡çš„å¼‚åŒç‚¹ï¼š

{papers}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œæ¯”è¾ƒï¼š
1. ç ”ç©¶ç›®æ ‡å’Œé—®é¢˜çš„å¼‚åŒ
2. æ–¹æ³•è®ºçš„åŒºåˆ«
3. ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹
4. ä¼˜ç¼ºç‚¹å¯¹æ¯”
5. é€‚ç”¨åœºæ™¯çš„å·®å¼‚

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œç»“æ„åŒ–è¾“å‡ºã€‚
"""

            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯æ¯”è¾ƒåˆ†æå¸ˆï¼Œæ“…é•¿æ·±å…¥åˆ†æå’Œæ¯”è¾ƒä¸åŒç ”ç©¶è®ºæ–‡ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.3
            )

            comparison = response.choices[0].message.content.strip()
            logger.info(f"æˆåŠŸå®Œæˆ {len(papers)} ç¯‡è®ºæ–‡çš„æ¯”è¾ƒåˆ†æ")
            return comparison

        except Exception as e:
            logger.error(f"è®ºæ–‡æ¯”è¾ƒåˆ†æå¤±è´¥: {e}")
            return "è®ºæ–‡æ¯”è¾ƒåˆ†ææ—¶å‡ºç°é”™è¯¯ã€‚"