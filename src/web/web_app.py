#!/usr/bin/env python3
"""
ç®€å•çš„Webç•Œé¢æ¥ç®¡ç†arXivçˆ¬è™«
"""

import os
from flask import Flask, render_template, request, jsonify, redirect, url_for, flash
import json
from datetime import datetime, timedelta
from pathlib import Path
from src.utils.logger import setup_logger, get_logger
from src.core.scheduler import PaperScheduler
from src.core.scraper import ArxivScraper
from src.core.analyzer import DeepSeekAnalyzer
from src.utils.utils import ConfigManager, PaperExporter, format_paper_summary
from config.settings import settings

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent.parent
template_folder = PROJECT_ROOT / "web" / "templates"
static_folder = PROJECT_ROOT / "web" / "static"

app = Flask(__name__, template_folder=str(template_folder), static_folder=str(static_folder))
app.secret_key = os.environ.get('FLASK_SECRET_KEY', 'arxiv-scraper-secret-key-change-in-production')

# æ·»åŠ è‡ªå®šä¹‰è¿‡æ»¤å™¨
@app.template_filter('nl2br')
def nl2br_filter(text):
    """å°†æ¢è¡Œç¬¦è½¬æ¢ä¸ºHTMLçš„<br>æ ‡ç­¾"""
    if text is None:
        return ''
    import re
    return re.sub(r'\r?\n', '<br>', text)

# åˆå§‹åŒ–ç»„ä»¶
scheduler = PaperScheduler()
scraper = ArxivScraper()
analyzer = DeepSeekAnalyzer()
config_manager = ConfigManager()
exporter = PaperExporter(scraper.db)

# è®¾ç½®æ—¥å¿—
setup_logger()
logger = get_logger(__name__)

# æ´å¯Ÿç¼“å­˜
_insights_cache = {}

def _get_cached_insights(cache_key):
    """è·å–ç¼“å­˜çš„æ´å¯Ÿæ•°æ®"""
    if cache_key in _insights_cache:
        cached_data = _insights_cache[cache_key]
        import time
        if time.time() - cached_data['timestamp'] < 3600:
            return cached_data
        else:
            # åˆ é™¤è¿‡æœŸç¼“å­˜
            del _insights_cache[cache_key]
    return None

def _cache_insights(cache_key, insights, trending):
    """ç¼“å­˜æ´å¯Ÿæ•°æ®"""
    import time
    _insights_cache[cache_key] = {
        'insights': insights,
        'trending': trending,
        'timestamp': time.time()
    }
    logger.info(f"å·²ç¼“å­˜æ´å¯Ÿæ•°æ®: {cache_key}")

@app.route('/refresh_insights')
def refresh_insights():

    days = int(request.args.get('days', 7))
    cache_key = f'insights_{days}'

    # åˆ é™¤ç¼“å­˜
    if cache_key in _insights_cache:
        del _insights_cache[cache_key]
        flash(f'å·²æ¸…é™¤ {days} å¤©çš„æ´å¯Ÿç¼“å­˜', 'success')
    else:
        flash('æ²¡æœ‰æ‰¾åˆ°ç¼“å­˜æ•°æ®', 'info')

    # å¯åŠ¨åå°ç”Ÿæˆæ–°çš„æ´å¯Ÿ
    try:
        import threading

        def background_regenerate():
            """åå°é‡æ–°ç”Ÿæˆæ´å¯Ÿ"""
            try:
                logger.info(f"åå°é‡æ–°ç”Ÿæˆæ´å¯Ÿå¼€å§‹: {cache_key}")
                insights_data = analyzer.get_research_insights(days)
                trending_data = scraper.get_trending_topics(days)
                _cache_insights(cache_key, insights_data, trending_data)
                logger.info(f"åå°æ´å¯Ÿé‡æ–°ç”Ÿæˆå®Œæˆ: {cache_key}")
            except Exception as e:
                logger.error(f"åå°æ´å¯Ÿé‡æ–°ç”Ÿæˆå¤±è´¥: {e}")
                _cache_insights(cache_key, f"é‡æ–°ç”Ÿæˆå¤±è´¥: {str(e)}", [])

        thread = threading.Thread(target=background_regenerate, daemon=True)
        thread.start()
        logger.info(f"å·²å¯åŠ¨åå°æ´å¯Ÿé‡æ–°ç”Ÿæˆ: {cache_key}")
        flash('æ­£åœ¨åå°é‡æ–°ç”Ÿæˆæ´å¯Ÿï¼Œè¯·ç¨ååˆ·æ–°é¡µé¢', 'info')

    except Exception as e:
        logger.error(f"å¯åŠ¨åå°æ´å¯Ÿé‡æ–°ç”Ÿæˆå¤±è´¥: {e}")

    return redirect(url_for('insights', days=days))

@app.route('/')
def index():
    """ä¸»é¡µ"""
    # è·å–ç³»ç»ŸçŠ¶æ€
    status = scheduler.get_status()
    recent_papers = scraper.db.get_recent_papers(7)

    return render_template('index.html',
                         status=status,
                         recent_papers=recent_papers[:5],
                         keywords=config_manager.get_keywords())

@app.route('/keywords', methods=['GET', 'POST'])
def keywords():
    """å…³é”®è¯ç®¡ç†"""
    if request.method == 'POST':
        action = request.form.get('action')

        if action == 'add':
            keyword = request.form.get('keyword', '').strip()
            if keyword:
                keywords = config_manager.get_keywords()
                if keyword not in keywords:
                    keywords.append(keyword)
                    config_manager.update_keywords(keywords)
                    flash(f'å·²æ·»åŠ å…³é”®è¯: {keyword}', 'success')
                else:
                    flash(f'å…³é”®è¯å·²å­˜åœ¨: {keyword}', 'warning')

        elif action == 'remove':
            keyword = request.form.get('keyword', '').strip()
            keywords = config_manager.get_keywords()
            if keyword in keywords:
                keywords.remove(keyword)
                config_manager.update_keywords(keywords)
                flash(f'å·²åˆ é™¤å…³é”®è¯: {keyword}', 'success')

        elif action == 'set':
            keywords_text = request.form.get('keywords', '').strip()
            if keywords_text:
                keywords = [k.strip() for k in keywords_text.split(',')]
                config_manager.update_keywords(keywords)
                flash(f'å…³é”®è¯å·²æ›´æ–°', 'success')

        return redirect(url_for('keywords'))

    keywords = config_manager.get_keywords()
    return render_template('keywords.html', keywords=keywords)

@app.route('/settings', methods=['GET', 'POST'])
def system_settings():
    """ç³»ç»Ÿé…ç½®ç®¡ç†"""
    if request.method == 'POST':
        action = request.form.get('action')

        try:
            if action == 'api_config':
                # æ›´æ–°APIé…ç½®
                deepseek_api_key = request.form.get('deepseek_api_key', '').strip()
                deepseek_base_url = request.form.get('deepseek_base_url', '').strip()

                # æ›´æ–°ç¯å¢ƒå˜é‡
                if deepseek_api_key:
                    os.environ['DEEPSEEK_API_KEY'] = deepseek_api_key
                    config_manager.update_config('DEEPSEEK_API_KEY', deepseek_api_key)

                if deepseek_base_url:
                    os.environ['DEEPSEEK_BASE_URL'] = deepseek_base_url
                    config_manager.update_config('DEEPSEEK_BASE_URL', deepseek_base_url)

                # é‡æ–°åˆå§‹åŒ–åˆ†æå™¨ä»¥ä½¿ç”¨æ–°çš„APIé…ç½®
                global analyzer
                analyzer = DeepSeekAnalyzer()

                flash('APIé…ç½®å·²æ›´æ–°', 'success')

            elif action == 'scraping_config':
                # æ›´æ–°çˆ¬å–é…ç½®
                max_papers = request.form.get('max_papers_per_day', '').strip()
                schedule_time = request.form.get('schedule_time', '').strip()

                if max_papers:
                    os.environ['MAX_PAPERS_PER_DAY'] = max_papers
                    config_manager.update_config('MAX_PAPERS_PER_DAY', max_papers)

                if schedule_time:
                    os.environ['SCHEDULE_TIME'] = schedule_time
                    config_manager.update_config('SCHEDULE_TIME', schedule_time)

                flash('çˆ¬å–é…ç½®å·²æ›´æ–°', 'success')

        except Exception as e:
            logger.error(f"æ›´æ–°é…ç½®å¤±è´¥: {e}")
            flash(f'æ›´æ–°é…ç½®å¤±è´¥: {str(e)}', 'error')

        return redirect(url_for('system_settings'))

    # è·å–å½“å‰é…ç½®
    config = {
        'deepseek_api_key': os.getenv('DEEPSEEK_API_KEY', ''),
        'deepseek_base_url': os.getenv('DEEPSEEK_BASE_URL', 'https://api.deepseek.com/v1'),
        'max_papers_per_day': os.getenv('MAX_PAPERS_PER_DAY', '10'),
        'schedule_time': os.getenv('SCHEDULE_TIME', '09:00'),
        'database_path': settings.database_path
    }

    # è·å–ç³»ç»ŸçŠ¶æ€
    status = scheduler.get_status()
    status['keywords_count'] = len(config_manager.get_keywords())

    return render_template('settings.html', config=config, status=status)

@app.route('/papers')
def papers():
    """è®ºæ–‡åˆ—è¡¨"""
    page = int(request.args.get('page', 1))
    per_page = 20
    search = request.args.get('search', '').strip()
    days = int(request.args.get('days', 30))

    if search:
        papers = scraper.db.search_papers(search)
    else:
        papers = scraper.db.get_recent_papers(days)

    # åˆ†é¡µ
    total = len(papers)
    start = (page - 1) * per_page
    end = start + per_page
    papers_page = papers[start:end]

    return render_template('papers.html',
                         papers=papers_page,
                         page=page,
                         per_page=per_page,
                         total=total,
                         search=search,
                         days=days)

@app.route('/paper/<arxiv_id>')
def paper_detail(arxiv_id):
    """è®ºæ–‡è¯¦æƒ…"""
    paper = scraper.db.get_paper_by_arxiv_id(arxiv_id)
    if paper:
        return render_template('paper_detail.html', paper=paper)
    else:
        flash(f'è®ºæ–‡æœªæ‰¾åˆ°: {arxiv_id}', 'error')
        return redirect(url_for('papers'))

@app.route('/scrape', methods=['POST'])
def scrape():
    """æ‰‹åŠ¨çˆ¬å–"""
    try:
        logger.info("Webç•Œé¢è§¦å‘æ‰‹åŠ¨çˆ¬å–")
        saved_count = scheduler.run_once()
        flash(f'çˆ¬å–å®Œæˆï¼Œä¿å­˜äº† {saved_count} ç¯‡æ–°è®ºæ–‡', 'success')
    except Exception as e:
        logger.error(f"Webç•Œé¢çˆ¬å–å¤±è´¥: {e}")
        flash(f'çˆ¬å–å¤±è´¥: {str(e)}', 'error')

    return redirect(url_for('index'))

@app.route('/scrape_more', methods=['POST'])
def scrape_more():
    """å¢é‡çˆ¬å–æ›´å¤šè®ºæ–‡"""
    try:
        logger.info("Webç•Œé¢è§¦å‘å¢é‡çˆ¬å–")
        additional_count = int(request.form.get('additional_count', 10))
        keywords = config_manager.get_keywords()

        saved_count = scraper.scrape_more_papers(keywords, additional_count)
        flash(f'å¢é‡çˆ¬å–å®Œæˆï¼Œé¢å¤–ä¿å­˜äº† {saved_count} ç¯‡æ–°è®ºæ–‡', 'success')

        # å¦‚æœä¿å­˜äº†æ–°è®ºæ–‡ï¼Œè‡ªåŠ¨ç”Ÿæˆæ‘˜è¦
        if saved_count > 0:
            logger.info(f"å¼€å§‹ä¸º {saved_count} ç¯‡æ–°è®ºæ–‡ç”ŸæˆAIæ‘˜è¦...")

            # è·å–ä»Šå¤©çš„è®ºæ–‡ï¼ˆåˆšçˆ¬å–çš„ï¼‰
            recent_papers = scraper.db.get_recent_papers(1)

            # åªåˆ†ææ²¡æœ‰æ‘˜è¦çš„è®ºæ–‡
            papers_needing_summary = [p for p in recent_papers if not p.summary]

            if papers_needing_summary:
                try:
                    analyzed_count = 0
                    for paper in papers_needing_summary:
                        logger.info(f"æ­£åœ¨ç”Ÿæˆè®ºæ–‡æ‘˜è¦: {paper.title[:50]}...")
                        summary = analyzer.generate_summary(paper)
                        if summary:
                            analyzer._update_paper_summary(paper.arxiv_id, summary)
                            analyzed_count += 1
                        # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                        import time
                        time.sleep(1)

                    logger.info(f"æˆåŠŸä¸º {analyzed_count} ç¯‡è®ºæ–‡ç”ŸæˆAIæ‘˜è¦")
                    flash(f'å·²ä¸º {analyzed_count} ç¯‡æ–°è®ºæ–‡ç”ŸæˆAIæ‘˜è¦', 'info')

                except Exception as e:
                    logger.error(f"ç”ŸæˆAIæ‘˜è¦å¤±è´¥: {e}")
                    flash(f'AIæ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}', 'warning')

            # è‡ªåŠ¨æ›´æ–°æ´å¯Ÿï¼ˆå¼‚æ­¥åå°æ‰§è¡Œï¼‰
            try:
                import threading
                import time

                def auto_update_insights():
                    """åå°è‡ªåŠ¨æ›´æ–°æ´å¯Ÿ"""
                    logger.info("å¼€å§‹åå°è‡ªåŠ¨æ›´æ–°ç ”ç©¶æ´å¯Ÿ...")
                    try:
                        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æ•°æ®åº“æ“ä½œå®Œæˆ
                        time.sleep(2)

                        # ç”Ÿæˆæ–°æ´å¯Ÿ
                        insights_data = analyzer.get_research_insights(7)
                        trending_data = analyzer.get_trending_topics(7)

                        # ç¼“å­˜æ–°æ´å¯Ÿ
                        cache_key = 'insights_7'
                        _cache_insights(cache_key, insights_data, trending_data)

                        logger.info("åå°æ´å¯Ÿæ›´æ–°å®Œæˆ")
                    except Exception as e:
                        logger.error(f"åå°æ´å¯Ÿæ›´æ–°å¤±è´¥: {e}")

                # å¯åŠ¨åå°çº¿ç¨‹
                insights_thread = threading.Thread(target=auto_update_insights, daemon=True)
                insights_thread.start()
                logger.info("å·²å¯åŠ¨åå°æ´å¯Ÿæ›´æ–°ä»»åŠ¡")

            except Exception as e:
                logger.error(f"å¯åŠ¨åå°æ´å¯Ÿæ›´æ–°å¤±è´¥: {e}")

            # æ¸…é™¤æ—§çš„æ´å¯Ÿç¼“å­˜
            global _insights_cache
            _insights_cache.clear()
            logger.info("å·²æ¸…é™¤æ´å¯Ÿç¼“å­˜")

    except Exception as e:
        logger.error(f"å¢é‡çˆ¬å–å¤±è´¥: {e}")
        flash(f'å¢é‡çˆ¬å–å¤±è´¥: {str(e)}', 'error')

    return redirect(url_for('index'))

@app.route('/scrape', methods=['GET'])
def scrape_get():
    """å¤„ç†GETè¯·æ±‚çš„scrapeï¼ˆé‡å®šå‘åˆ°é¦–é¡µï¼‰"""
    flash('è¯·ä½¿ç”¨è¡¨å•æäº¤æ¥æ‰§è¡Œçˆ¬å–æ“ä½œ', 'warning')
    return redirect(url_for('index'))

@app.route('/api/insights_status')
def api_insights_status():
    """è·å–æ´å¯ŸçŠ¶æ€API"""
    days = int(request.args.get('days', 7))
    cache_key = f'insights_{days}'

    cached_insights = _get_cached_insights(cache_key)

    status = {
        'cache_key': cache_key,
        'has_cache': cached_insights is not None,
        'last_updated': cached_insights.get('timestamp') if cached_insights else None,
        'is_generating': cache_key in _insights_cache and cached_insights is None
    }

    return jsonify(status)

@app.route('/insights')
def insights():
    """ç ”ç©¶æ´å¯Ÿ"""
    days = int(request.args.get('days', 7))

    # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æ´å¯Ÿ
    cache_key = f'insights_{days}'
    cached_insights = _get_cached_insights(cache_key)

    if cached_insights:
        logger.info(f"ä½¿ç”¨ç¼“å­˜çš„æ´å¯Ÿæ•°æ®: {cache_key}")
        insights = cached_insights['insights']
        trending = cached_insights['trending']
    else:
        # å¼‚æ­¥ç”Ÿæˆæ´å¯Ÿï¼Œé¿å…é¡µé¢å¡ä½
        try:
            logger.info(f"ç”Ÿæˆæ–°çš„æ´å¯Ÿæ•°æ®: {cache_key}")

            # ä½¿ç”¨çº¿ç¨‹æ¥å¤„ç†å¯èƒ½è€—æ—¶çš„æ“ä½œ
            import threading
            import queue

            result_queue = queue.Queue()

            def generate_insights():
                try:
                    insights_data = analyzer.get_research_insights(days)
                    trending_data = scraper.get_trending_topics(days)
                    result_queue.put(('success', insights_data, trending_data))
                except Exception as e:
                    result_queue.put(('error', str(e), []))

            # å¯åŠ¨çº¿ç¨‹
            thread = threading.Thread(target=generate_insights)
            thread.daemon = True
            thread.start()

            # ç­‰å¾…ç»“æœï¼Œæœ€å¤šç­‰å¾…30ç§’
            thread.join(timeout=30)

            if thread.is_alive():
                logger.warning("æ´å¯Ÿç”Ÿæˆè¶…æ—¶ï¼Œä½¿ç”¨é»˜è®¤å†…å®¹")
                insights = "æ´å¯Ÿç”Ÿæˆè¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•ã€‚"
                trending = []
            else:
                try:
                    status, insights_data, trending_data = result_queue.get_nowait()
                    if status == 'success':
                        insights = insights_data
                        trending = trending_data
                        # ç¼“å­˜ç»“æœ
                        _cache_insights(cache_key, insights, trending)
                    else:
                        insights = f"ç”Ÿæˆæ´å¯Ÿå¤±è´¥: {insights_data}"
                        trending = []
                except queue.Empty:
                    logger.warning("æ´å¯Ÿç”Ÿæˆé˜Ÿåˆ—ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å†…å®¹")
                    insights = "æ´å¯Ÿç”Ÿæˆå¼‚å¸¸ï¼Œè¯·ç¨åé‡è¯•ã€‚"
                    trending = []

        except Exception as e:
            logger.error(f"ç”Ÿæˆæ´å¯Ÿå¤±è´¥: {e}")
            insights = f"ç”Ÿæˆæ´å¯Ÿå¤±è´¥: {str(e)}"
            trending = []

    return render_template('insights.html',
                         insights=insights,
                         trending=trending,
                         days=days)

@app.route('/compare', methods=['GET', 'POST'])
def compare():
    """è®ºæ–‡æ¯”è¾ƒ"""
    if request.method == 'POST':
        paper_ids = request.form.get('paper_ids', '').strip().split()
        paper_ids = [pid.strip() for pid in paper_ids if pid.strip()]

        if len(paper_ids) < 2:
            flash('è¯·æä¾›è‡³å°‘ä¸¤ä¸ªè®ºæ–‡ID', 'error')
        else:
            try:
                comparison = analyzer.compare_papers(paper_ids)
                return render_template('compare_result.html',
                                     comparison=comparison,
                                     paper_ids=paper_ids)
            except Exception as e:
                flash(f'æ¯”è¾ƒå¤±è´¥: {str(e)}', 'error')

    return render_template('compare.html')

@app.route('/export')
def export():
    """å¯¼å‡ºæ•°æ®"""
    export_format = request.args.get('format', 'json')
    days = int(request.args.get('days', 30))

    # å¦‚æœdaysä¸º-1ï¼Œè·å–æ‰€æœ‰æ•°æ®
    if days == -1:
        # è·å–æ‰€æœ‰è®ºæ–‡ï¼ˆéœ€è¦ä¿®æ”¹æ•°æ®åº“æ–¹æ³•æ¥æ”¯æŒè·å–æ‰€æœ‰æ•°æ®ï¼‰
        papers = scraper.db.get_all_papers() if hasattr(scraper.db, 'get_all_papers') else scraper.db.get_recent_papers(3650)  # 10å¹´ä½œä¸º"å…¨éƒ¨"
    else:
        papers = scraper.db.get_recent_papers(days)

    if not papers:
        flash('æ²¡æœ‰æ•°æ®å¯å¯¼å‡º', 'warning')
        return redirect(url_for('papers'))

    try:
        if export_format == 'json':
            filepath = exporter.export_to_json(papers)
        elif export_format == 'markdown':
            filepath = exporter.export_to_markdown(papers)
        elif export_format == 'bibtex':
            filepath = exporter.export_to_bibtex(papers)
        else:
            flash('ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼', 'error')
            return redirect(url_for('papers'))

        if filepath:
            flash(f'æ•°æ®å·²å¯¼å‡ºåˆ°: {filepath}', 'success')
        else:
            flash('å¯¼å‡ºå¤±è´¥', 'error')

    except Exception as e:
        flash(f'å¯¼å‡ºå¤±è´¥: {str(e)}', 'error')

    return redirect(url_for('papers'))

@app.route('/export_page')
def export_page():
    """å¯¼å‡ºé¡µé¢"""
    return render_template('export.html')

@app.route('/api/status')
def api_status():
    """API: è·å–ç³»ç»ŸçŠ¶æ€"""
    return jsonify(scheduler.get_status())

@app.route('/api/papers')
def api_papers():
    """API: è·å–è®ºæ–‡åˆ—è¡¨"""
    search = request.args.get('search', '').strip()
    days = int(request.args.get('days', 30))

    if search:
        papers = scraper.db.search_papers(search)
    else:
        papers = scraper.db.get_recent_papers(days)

    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    papers_data = []
    for paper in papers:
        papers_data.append({
            'title': paper.title,
            'authors': paper.authors,
            'abstract': paper.abstract[:200] + '...',
            'arxiv_id': paper.arxiv_id,
            'published_date': paper.published_date.isoformat() if paper.published_date else None,
            'categories': paper.categories,
            'summary': paper.summary[:200] + '...' if paper.summary else None
        })

    return jsonify(papers_data)

@app.route('/api/insights')
def api_insights():
    """API: è·å–ç ”ç©¶æ´å¯Ÿ"""
    days = int(request.args.get('days', 7))
    try:
        insights = analyzer.get_research_insights(days)
        trending = scraper.get_trending_topics(days)
        return jsonify({
            'insights': insights,
            'trending': trending
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/paper/<arxiv_id>/bibtex')
def api_paper_bibtex(arxiv_id):
    """API: è·å–å•ä¸ªè®ºæ–‡çš„BibTeXæ ¼å¼"""
    try:
        paper = scraper.db.get_paper_by_arxiv_id(arxiv_id)
        if not paper:
            return jsonify({'error': 'è®ºæ–‡æœªæ‰¾åˆ°'}), 404

        # ç”ŸæˆBibTeX key
        first_author_lastname = paper.authors[0].split()[-1] if paper.authors else "Unknown"
        year = paper.published_date.year if paper.published_date else datetime.now().year
        title_words = paper.title.split()[:3]
        title_key = ''.join([word.strip('.,!?;:') for word in title_words])
        bibtex_key = f"{first_author_lastname}{year}{title_key}"

        # æ¸…ç†å¹¶æ ¼å¼åŒ–æ•°æ®
        title = paper.title.replace('{', '\\{').replace('}', '\\}').replace('&', '\\&')
        authors = ' and '.join(paper.authors)
        abstract = paper.abstract.replace('{', '\\{').replace('}', '\\}').replace('\n', ' ')

        # ç”ŸæˆBibTeXæ¡ç›®
        bibtex_entry = f"""@misc{{{bibtex_key},
  title = {{{title}}},
  author = {{{authors}}},
  year = {{{year}}},
  eprint = {{{paper.arxiv_id}}},
  archivePrefix = {{arXiv}},
  primaryClass = {{{paper.categories[0] if paper.categories else 'cs.AI'}}},"""

        if abstract:
            bibtex_entry += f"""
  abstract = {{{abstract}}},"""

        bibtex_entry += f"""
  url = {{{paper.pdf_url}}},
  howpublished = {{arXiv:{paper.arxiv_id}}}
}}"""

        return bibtex_entry

    except Exception as e:
        logger.error(f"ç”ŸæˆBibTeXå¤±è´¥: {e}")
        return jsonify({'error': f'ç”ŸæˆBibTeXå¤±è´¥: {str(e)}'}), 500

if __name__ == '__main__':
    # åˆ›å»ºæ¨¡æ¿ç›®å½•
    import os
    os.makedirs('templates', exist_ok=True)
    os.makedirs('static', exist_ok=True)

    print("ğŸŒ å¯åŠ¨Webç•Œé¢...")
    print("ğŸ“± è®¿é—®åœ°å€: http://localhost:5000")
    print("ğŸ’¡ ä½¿ç”¨ Ctrl+C åœæ­¢æœåŠ¡")

    app.run(host='0.0.0.0', port=5000, debug=True)