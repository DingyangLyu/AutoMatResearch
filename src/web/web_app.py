#!/usr/bin/env python3
"""
ç®€å•çš„Webç•Œé¢æ¥ç®¡ç†arXivçˆ¬è™«
"""

import os
import sys
from flask import Flask, render_template, request, jsonify, redirect, url_for, flash
import json
import markdown
from datetime import datetime, timedelta
from pathlib import Path
import schedule

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.utils.logger import setup_logger, get_logger
from src.core.scheduler import PaperScheduler
from src.core.scraper import ArxivScraper
from src.core.analyzer import DeepSeekAnalyzer
from src.utils.utils import ConfigManager, PaperExporter, format_paper_summary
from src.data.keyword_manager import keyword_manager
from config.settings import settings

# è®¾ç½®æ¨¡æ¿å’Œé™æ€æ–‡ä»¶è·¯å¾„
template_folder = PROJECT_ROOT / "web" / "templates"
static_folder = PROJECT_ROOT / "web" / "static"

app = Flask(__name__, template_folder=str(template_folder), static_folder=str(static_folder))
app.secret_key = os.environ.get('FLASK_SECRET_KEY', 'arxiv-scraper-secret-key-change-in-production')

# æ·»åŠ è‡ªå®šä¹‰è¿‡æ»¤å™¨
@app.template_filter('nl2br')
def nl2br_filter(text):
    """å°†æ¢è¡Œç¬¦è½¬æ¢ä¸ºHTMLçš„<br>æ ‡ç­¾"""
    if text is None:
        return ''
    import re
    return re.sub(r'\r?\n', '<br>', text)

# åˆå§‹åŒ–ç»„ä»¶
scheduler = PaperScheduler()
config_manager = ConfigManager()

# å…¨å±€å˜é‡è·Ÿè¸ªåå°ä»»åŠ¡çŠ¶æ€
background_tasks = {
    'ai_analysis_running': False,
    'ai_analysis_progress': 0,
    'ai_analysis_total': 0,
    'ai_analysis_start_time': None
}

# å‡½æ•°ï¼šè·å–å½“å‰å…³é”®è¯çš„ç»„ä»¶
def get_current_components():
    """è·å–å½“å‰å…³é”®è¯çš„scraperã€analyzerç­‰ç»„ä»¶"""
    current_keyword = keyword_manager.get_current_keyword()
    scraper = ArxivScraper(current_keyword)
    analyzer = DeepSeekAnalyzer(current_keyword)
    exporter = PaperExporter(scraper.db)
    return scraper, analyzer, exporter

# é»˜è®¤ç»„ä»¶ï¼ˆç”¨äºåˆå§‹åŒ–ï¼‰
scraper, analyzer, exporter = get_current_components()

# è®¾ç½®æ—¥å¿—
setup_logger()
logger = get_logger(__name__)

# ä¸Šä¸‹æ–‡å¤„ç†å™¨ï¼Œç¡®ä¿æ‰€æœ‰æ¨¡æ¿éƒ½èƒ½è®¿é—®å…³é”®è¯ä¿¡æ¯
@app.context_processor
def inject_keywords():
    """å‘æ‰€æœ‰æ¨¡æ¿æ³¨å…¥å…³é”®è¯ä¿¡æ¯"""
    current_config = keyword_manager.get_current_config()
    all_keywords = keyword_manager.get_all_keywords()
    return dict(
        current_keyword=current_config,
        all_keywords=all_keywords
    )

# æ´å¯Ÿç¼“å­˜ç°åœ¨ä½¿ç”¨æ•°æ®åº“æ°¸ä¹…ç¼“å­˜ï¼Œæ— éœ€å†…å­˜ç¼“å­˜

@app.route('/refresh_insights', methods=['GET', 'POST'])
def refresh_insights():
    """å¼ºåˆ¶åˆ·æ–°æ´å¯Ÿç¼“å­˜"""
    if request.method == 'POST':
        days = int(request.form.get('days', 7))
    else:
        days = int(request.args.get('days', 7))

    try:
        # æ¸…é™¤æ•°æ®åº“ç¼“å­˜
        import sqlite3
        with sqlite3.connect(scraper.db.db_path) as conn:
            conn.execute("DELETE FROM insights_cache WHERE cache_key = ?", (f'insights_{days}',))
            conn.commit()
        logger.info(f"å·²æ¸…é™¤ {days} å¤©çš„æ•°æ®åº“æ´å¯Ÿç¼“å­˜")

        flash(f'æ­£åœ¨é‡æ–°ç”Ÿæˆ {days} å¤©æ´å¯Ÿ...', 'info')

        # åŒæ­¥ç”Ÿæˆæ–°çš„æ´å¯Ÿï¼Œç¡®ä¿ç”¨æˆ·èƒ½çœ‹åˆ°æœ€æ–°çš„æ´å¯Ÿ
        try:
            logger.info(f"é‡æ–°ç”Ÿæˆæ´å¯Ÿå¼€å§‹: insights_{days}")
            new_insights = analyzer.get_research_insights(days)
            logger.info(f"æ´å¯Ÿé‡æ–°ç”Ÿæˆå®Œæˆ: insights_{days}")
            flash('æ´å¯Ÿå·²æˆåŠŸæ›´æ–°ï¼', 'success')
        except Exception as e:
            logger.error(f"æ´å¯Ÿé‡æ–°ç”Ÿæˆå¤±è´¥: {e}")
            flash(f"ç”Ÿæˆæ´å¯Ÿå¤±è´¥: {str(e)}", 'error')

    except Exception as e:
        logger.error(f"æ¸…é™¤æ´å¯Ÿç¼“å­˜å¤±è´¥: {e}")
        flash(f"æ“ä½œå¤±è´¥: {str(e)}", 'error')

    return redirect(url_for('insights', days=days))

@app.route('/set_keyword', methods=['POST'])
def set_keyword():
    """åˆ‡æ¢å½“å‰å…³é”®è¯"""
    keyword = request.form.get('keyword', '').strip()
    if keyword and keyword_manager.set_current_keyword(keyword):
        flash(f'å·²åˆ‡æ¢åˆ°å…³é”®è¯: {keyword_manager.get_current_config().display_name}', 'success')
        logger.info(f"å…³é”®è¯åˆ‡æ¢åˆ°: {keyword}")
    else:
        flash('åˆ‡æ¢å…³é”®è¯å¤±è´¥', 'error')
        logger.error(f"åˆ‡æ¢å…³é”®è¯å¤±è´¥: {keyword}")

    # è¿”å›åˆ°ä¹‹å‰çš„é¡µé¢
    return redirect(request.referrer or url_for('index'))

@app.route('/add_keyword_auto', methods=['POST'])
def add_keyword_auto():
    """ä½¿ç”¨è‡ªåŠ¨æŸ¥è¯¢ç”Ÿæˆæ·»åŠ æ–°å…³é”®è¯"""
    name = request.form.get('name', '').strip().replace(' ', '_').lower()
    display_name = request.form.get('display_name', '').strip()
    user_keywords = request.form.get('user_keywords', '').strip()

    # è·å–æœç´¢å­—æ®µé€‰é¡¹
    search_fields = request.form.getlist('search_fields')
    use_synonyms = request.form.get('use_synonyms') == 'on'
    use_categories = request.form.get('use_categories') == 'on'

    if not name or not display_name or not user_keywords:
        flash('è¯·å¡«å†™å®Œæ•´çš„å…³é”®è¯ä¿¡æ¯', 'error')
        return redirect(url_for('keywords'))

    # ä½¿ç”¨è‡ªåŠ¨æŸ¥è¯¢ç”Ÿæˆ
    success, generated_query = keyword_manager.add_keyword_auto(
        name=name,
        display_name=display_name,
        user_keywords=user_keywords,
        search_fields=search_fields or ['all'],
        use_categories=use_categories
    )

    if success:
        flash(f'å·²æ·»åŠ å…³é”®è¯: {display_name} (æŸ¥è¯¢: {generated_query})', 'success')
        logger.info(f"è‡ªåŠ¨æ·»åŠ å…³é”®è¯: {name} ({display_name}) - æŸ¥è¯¢: {generated_query}")
    else:
        flash(f'å…³é”®è¯å·²å­˜åœ¨æˆ–æ·»åŠ å¤±è´¥: {display_name}', 'error')

    return redirect(url_for('keywords'))

@app.route('/add_keyword', methods=['POST'])
def add_keyword():
    """æ·»åŠ æ–°å…³é”®è¯ï¼ˆæ‰‹åŠ¨æ¨¡å¼ï¼‰"""
    name = request.form.get('name', '').strip().replace(' ', '_').lower()
    display_name = request.form.get('display_name', '').strip()
    search_query = request.form.get('search_query', '').strip()

    if not name or not display_name or not search_query:
        flash('è¯·å¡«å†™å®Œæ•´çš„å…³é”®è¯ä¿¡æ¯', 'error')
        return redirect(url_for('keywords'))

    if keyword_manager.add_keyword(name, display_name, search_query):
        flash(f'å·²æ·»åŠ å…³é”®è¯: {display_name}', 'success')
        logger.info(f"æ·»åŠ å…³é”®è¯: {name} ({display_name})")
    else:
        flash(f'å…³é”®è¯å·²å­˜åœ¨æˆ–æ·»åŠ å¤±è´¥: {display_name}', 'error')

    return redirect(url_for('keywords'))

@app.route('/remove_keyword', methods=['POST'])
def remove_keyword():
    """åˆ é™¤å…³é”®è¯"""
    keyword = request.form.get('keyword', '').strip()
    current_keyword = keyword_manager.get_current_keyword()

    if keyword == current_keyword:
        flash('ä¸èƒ½åˆ é™¤å½“å‰ä½¿ç”¨çš„å…³é”®è¯', 'error')
        return redirect(url_for('keywords'))

    config = keyword_manager.get_keyword_config(keyword)
    display_name = config.display_name if config else keyword

    if keyword_manager.remove_keyword(keyword):
        flash(f'å·²åˆ é™¤å…³é”®è¯: {display_name}', 'success')
        logger.info(f"åˆ é™¤å…³é”®è¯: {keyword}")
    else:
        flash(f'åˆ é™¤å…³é”®è¯å¤±è´¥: {display_name}', 'error')

    return redirect(url_for('keywords'))

@app.route('/')
def index():
    """ä¸»é¡µ"""
    # è·å–å½“å‰ç»„ä»¶
    current_scraper, current_analyzer, current_exporter = get_current_components()

    # è·å–å½“å‰å…³é”®è¯ä¿¡æ¯
    current_config = keyword_manager.get_current_config()
    all_keywords = keyword_manager.get_all_keywords()

    # è·å–å½“å‰å…³é”®è¯çš„å®æ—¶çŠ¶æ€ï¼ˆæ›¿ä»£è°ƒåº¦å™¨çš„å›ºå®šçŠ¶æ€ï¼‰
    current_papers_7d = current_scraper.db.get_recent_papers(7)
    current_papers_count = len(current_papers_7d)

    # æ„å»ºåŠ¨æ€çŠ¶æ€ä¿¡æ¯
    status = {
        'is_running': scheduler.is_running,
        'next_run': schedule.next_run() if hasattr(schedule, 'next_run') else None,
        'recent_papers_count': current_papers_count,  # å½“å‰å…³é”®è¯çš„7å¤©è®ºæ–‡æ•°
        'keywords': [current_config.display_name],  # å½“å‰å…³é”®è¯
        'schedule_time': settings.SCHEDULE_TIME,
        'max_papers_per_day': settings.MAX_PAPERS_PER_DAY
    }

    # è·å–æœ€æ–°10ç¯‡è®ºæ–‡ï¼ˆä¸é™åˆ¶æ—¶é—´èŒƒå›´ï¼Œæ˜¾ç¤ºæœ€æ–°çš„ç ”ç©¶æˆæœï¼‰
    recent_papers = current_scraper.db.get_recent_papers(30)  # è·å–æ›´å¤šè®ºæ–‡ç”¨äºç­›é€‰
    # æŒ‰å‘è¡¨æ—¶é—´æ’åºå¹¶å–æœ€æ–°çš„10ç¯‡
    recent_papers.sort(key=lambda p: p.published_date, reverse=True)
    recent_papers = recent_papers[:10]

    return render_template('index.html',
                         status=status,
                         recent_papers=recent_papers,
                         current_keyword=current_config,
                         all_keywords=all_keywords)

@app.route('/keywords', methods=['GET'])
def keywords():
    """å…³é”®è¯ç®¡ç†é¡µé¢"""
    all_keywords = keyword_manager.get_all_keywords()
    current_config = keyword_manager.get_current_config()

    return render_template('keywords_simple.html',
                         keywords=all_keywords,
                         current_keyword=keyword_manager.get_current_keyword(),
                         current_keyword_config=current_config)

@app.route('/keywords_simple', methods=['GET'])
def keywords_simple():
    """ç®€åŒ–å…³é”®è¯ç®¡ç†é¡µé¢"""
    all_keywords = keyword_manager.get_all_keywords()
    current_config = keyword_manager.get_current_config()

    return render_template('keywords_simple.html',
                         keywords=all_keywords,
                         current_keyword=keyword_manager.get_current_keyword(),
                         current_keyword_config=current_config)

@app.route('/add_keyword_multi', methods=['POST'])
def add_keyword_multi():
    """æ·»åŠ å¤šå…³é”®è¯"""
    try:
        name = request.form.get('name', '').strip()
        display_name = request.form.get('display_name', '').strip()
        keywords_str = request.form.get('keywords', '').strip()
        logic = request.form.get('logic', 'AND').strip()

        if not name or not display_name or not keywords_str:
            flash('è¯·å¡«å†™æ‰€æœ‰å¿…å¡«å­—æ®µ', 'error')
            return redirect(url_for('keywords'))

        # è§£æå…³é”®è¯åˆ—è¡¨
        keywords_list = [kw.strip() for kw in keywords_str.split(',') if kw.strip()]

        if len(keywords_list) == 0:
            flash('è¯·è‡³å°‘æ·»åŠ ä¸€ä¸ªå…³é”®è¯', 'error')
            return redirect(url_for('keywords'))

        # æ·»åŠ å¤šå…³é”®è¯
        success, generated_query = keyword_manager.add_keyword_multi(
            name=name,
            display_name=display_name,
            keywords=keywords_list,
            logic=logic,
            use_categories='use_categories' in request.form
        )

        if success:
            flash(f'å…³é”®è¯ "{display_name}" æ·»åŠ æˆåŠŸï¼', 'success')
            flash(f'ç”Ÿæˆçš„æŸ¥è¯¢: {generated_query}', 'info')
        else:
            flash('å…³é”®è¯åç§°å·²å­˜åœ¨', 'error')

    except Exception as e:
        app.logger.error(f"æ·»åŠ å…³é”®è¯å¤±è´¥: {e}")
        flash(f'æ·»åŠ å…³é”®è¯å¤±è´¥: {str(e)}', 'error')

    return redirect(url_for('keywords'))

@app.route('/settings', methods=['GET', 'POST'])
def system_settings():
    """ç³»ç»Ÿé…ç½®ç®¡ç†"""
    if request.method == 'POST':
        action = request.form.get('action')

        try:
            if action == 'api_config':
                # æ›´æ–°APIé…ç½®
                deepseek_api_key = request.form.get('deepseek_api_key', '').strip()
                deepseek_base_url = request.form.get('deepseek_base_url', '').strip()

                # æ›´æ–°ç¯å¢ƒå˜é‡
                if deepseek_api_key:
                    os.environ['DEEPSEEK_API_KEY'] = deepseek_api_key
                    config_manager.update_config('DEEPSEEK_API_KEY', deepseek_api_key)

                if deepseek_base_url:
                    os.environ['DEEPSEEK_BASE_URL'] = deepseek_base_url
                    config_manager.update_config('DEEPSEEK_BASE_URL', deepseek_base_url)

                # é‡æ–°åˆå§‹åŒ–åˆ†æå™¨ä»¥ä½¿ç”¨æ–°çš„APIé…ç½®
                global analyzer
                analyzer = DeepSeekAnalyzer()

                flash('APIé…ç½®å·²æ›´æ–°', 'success')

            elif action == 'scraping_config':
                # æ›´æ–°çˆ¬å–é…ç½®
                max_papers = request.form.get('max_papers_per_day', '').strip()
                schedule_time = request.form.get('schedule_time', '').strip()

                if max_papers:
                    os.environ['MAX_PAPERS_PER_DAY'] = max_papers
                    config_manager.update_config('MAX_PAPERS_PER_DAY', max_papers)

                if schedule_time:
                    os.environ['SCHEDULE_TIME'] = schedule_time
                    config_manager.update_config('SCHEDULE_TIME', schedule_time)

                flash('çˆ¬å–é…ç½®å·²æ›´æ–°', 'success')

        except Exception as e:
            logger.error(f"æ›´æ–°é…ç½®å¤±è´¥: {e}")
            flash(f'æ›´æ–°é…ç½®å¤±è´¥: {str(e)}', 'error')

        return redirect(url_for('system_settings'))

    # è·å–å½“å‰é…ç½®
    config = {
        'deepseek_api_key': os.getenv('DEEPSEEK_API_KEY', ''),
        'deepseek_base_url': os.getenv('DEEPSEEK_BASE_URL', 'https://api.deepseek.com/v1'),
        'max_papers_per_day': os.getenv('MAX_PAPERS_PER_DAY', '10'),
        'schedule_time': os.getenv('SCHEDULE_TIME', '09:00'),
        'database_path': settings.database_path
    }

    # è·å–ç³»ç»ŸçŠ¶æ€
    status = scheduler.get_status()
    status['keywords_count'] = len(config_manager.get_keywords())

    # æ·»åŠ æ€»è®ºæ–‡æ•°
    try:
        current_scraper, _, _ = get_current_components()
        status['total_papers'] = current_scraper.db.get_total_papers_count()
    except Exception as e:
        print(f"Error getting total papers count: {e}")
        status['total_papers'] = 0

    return render_template('settings.html', config=config, status=status)

@app.route('/papers')
def papers():
    """è®ºæ–‡åˆ—è¡¨"""
    # è·å–å½“å‰å…³é”®è¯çš„ç»„ä»¶
    current_scraper, current_analyzer, current_exporter = get_current_components()

    page = int(request.args.get('page', 1))
    per_page = 20
    search = request.args.get('search', '').strip()
    days = int(request.args.get('days', 30))

    if search:
        papers = current_scraper.db.search_papers(search)
    elif days == 0:  # 0 è¡¨ç¤ºæ‰€æœ‰æ—¶é—´
        papers = current_scraper.db.get_all_papers()
    else:
        papers = current_scraper.db.get_recent_papers(days)

    # åˆ†é¡µ
    total = len(papers)
    start = (page - 1) * per_page
    end = start + per_page
    papers_page = papers[start:end]

    return render_template('papers.html',
                         papers=papers_page,
                         page=page,
                         per_page=per_page,
                         total=total,
                         search=search,
                         days=days)

@app.route('/paper/<arxiv_id>')
def paper_detail(arxiv_id):
    """è®ºæ–‡è¯¦æƒ…"""
    # è·å–å½“å‰å…³é”®è¯çš„ç»„ä»¶
    current_scraper, current_analyzer, current_exporter = get_current_components()
    paper = current_scraper.db.get_paper_by_arxiv_id(arxiv_id)
    if paper:
        return render_template('paper_detail.html', paper=paper)
    else:
        flash(f'è®ºæ–‡æœªæ‰¾åˆ°: {arxiv_id}', 'error')
        return redirect(url_for('papers'))

@app.route('/scrape', methods=['POST'])
def scrape():
    """æ‰‹åŠ¨çˆ¬å–"""
    try:
        logger.info("Webç•Œé¢è§¦å‘æ‰‹åŠ¨çˆ¬å–")
        saved_count = scheduler.run_once()
        flash(f'çˆ¬å–å®Œæˆï¼Œä¿å­˜äº† {saved_count} ç¯‡æ–°è®ºæ–‡', 'success')
    except Exception as e:
        logger.error(f"Webç•Œé¢çˆ¬å–å¤±è´¥: {e}")
        flash(f'çˆ¬å–å¤±è´¥: {str(e)}', 'error')

    return redirect(url_for('index'))

@app.route('/scrape_more', methods=['POST'])
def scrape_more():
    """å¢é‡çˆ¬å–æ›´å¤šè®ºæ–‡"""
    try:
        logger.info("Webç•Œé¢è§¦å‘å¢é‡çˆ¬å–")

        # éªŒè¯ç”¨æˆ·è¾“å…¥çš„æ•°é‡
        try:
            additional_count = int(request.form.get('additional_count', 10))
        except (ValueError, TypeError):
            additional_count = 10

        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        if additional_count < 1:
            additional_count = 1
        elif additional_count > 200:
            additional_count = 200
            flash('è¯·æ±‚æ•°é‡è¶…è¿‡æœ€å¤§é™åˆ¶ï¼Œå·²è°ƒæ•´ä¸º200ç¯‡', 'warning')

        logger.info(f"ç”¨æˆ·è¯·æ±‚çˆ¬å– {additional_count} ç¯‡è®ºæ–‡")

        # è·å–å½“å‰å…³é”®è¯çš„ç»„ä»¶å’Œé…ç½®
        current_scraper, current_analyzer, current_exporter = get_current_components()
        current_config = keyword_manager.get_current_config()

        # ä½¿ç”¨å½“å‰å…³é”®è¯çš„æœç´¢æŸ¥è¯¢
        search_query = current_config.search_query
        keywords = [search_query]  # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ä»¥å…¼å®¹ç°æœ‰æ¥å£

        logger.info(f"ä½¿ç”¨å½“å‰å…³é”®è¯è¿›è¡Œçˆ¬å–: {current_config.display_name} ({search_query})")
        saved_count = current_scraper.scrape_more_papers(keywords, additional_count)
        flash(f'å¢é‡çˆ¬å–å®Œæˆï¼Œé¢å¤–ä¿å­˜äº† {saved_count} ç¯‡æ–°è®ºæ–‡', 'success')

        # å¦‚æœä¿å­˜äº†æ–°è®ºæ–‡ï¼Œè‡ªåŠ¨ç”Ÿæˆæ‘˜è¦
        if saved_count > 0:
            logger.info(f"å¼€å§‹ä¸ºæ²¡æœ‰æ‘˜è¦çš„è®ºæ–‡ç”ŸæˆAIæ‘˜è¦...")

            # è·å–æ‰€æœ‰æ²¡æœ‰æ‘˜è¦çš„è®ºæ–‡
            papers_needing_summary = current_scraper.db.get_papers_without_summary()
            logger.info(f"æ‰¾åˆ° {len(papers_needing_summary)} ç¯‡è®ºæ–‡éœ€è¦ç”ŸæˆAIæ‘˜è¦ï¼ˆæ€»å…±ä¿å­˜äº† {saved_count} ç¯‡æ–°è®ºæ–‡ï¼‰")

            
            if papers_needing_summary:
                # è®¾ç½®åå°ä»»åŠ¡çŠ¶æ€
                background_tasks['ai_analysis_running'] = True
                background_tasks['ai_analysis_progress'] = 0
                background_tasks['ai_analysis_total'] = len(papers_needing_summary)
                background_tasks['ai_analysis_start_time'] = datetime.now()

                # è·å–å½“å‰å…³é”®è¯é…ç½®å’Œç»„ä»¶ï¼Œä¼ é€’ç»™åå°çº¿ç¨‹
                current_keyword = keyword_manager.get_current_keyword()
                current_config = keyword_manager.get_current_config()
                papers_to_process = papers_needing_summary.copy()  # åˆ›å»ºå‰¯æœ¬é¿å…é—­åŒ…é—®é¢˜

                # å¯åŠ¨åå°ä»»åŠ¡å¤„ç†AIæ‘˜è¦ç”Ÿæˆ
                def background_ai_analysis():
                    try:
                        # åœ¨åå°çº¿ç¨‹ä¸­é‡æ–°åˆå§‹åŒ–ç»„ä»¶
                        from src.core.scraper import ArxivScraper
                        from src.core.analyzer import DeepSeekAnalyzer

                        # ä½¿ç”¨å½“å‰å…³é”®è¯åˆå§‹åŒ–scraperå’Œanalyzerï¼Œç¡®ä¿æ•°æ®åº“è·¯å¾„æ­£ç¡®
                        scraper = ArxivScraper(keyword=current_keyword)
                        analyzer = DeepSeekAnalyzer(keyword=current_keyword)
                        global_analyzer = analyzer  # ä½¿ç”¨æœ¬åœ°å®ä¾‹

                        logger.info(f"ğŸ”— Background thread DB paths:")
                        logger.info(f"  Scraper: {scraper.db.db_path}")
                        logger.info(f"  Analyzer: {analyzer.db.db_path}")
                        logger.info(f"  Config: {current_config.db_path}")

                        analyzed_count = 0
                        total_count = len(papers_to_process)

                        logger.info(f"ğŸ”„ åå°çº¿ç¨‹å·²å¯åŠ¨ï¼Œéœ€è¦å¤„ç† {total_count} ç¯‡è®ºæ–‡")

                        for i, paper in enumerate(papers_to_process):
                            logger.info(f"æ­£åœ¨ç”Ÿæˆè®ºæ–‡æ‘˜è¦ ({i+1}/{total_count}): {paper.title[:50]}...")
                            try:
                                summary = analyzer.generate_summary(paper)
                                if summary:
                                    analyzer._update_paper_summary(paper.arxiv_id, summary)
                                    analyzed_count += 1
                                    logger.info(f"âœ… å®Œæˆç¬¬ {i+1}/{total_count} ç¯‡è®ºæ–‡æ‘˜è¦")
                                else:
                                    logger.warning(f"âŒ ç¬¬ {i+1}/{total_count} ç¯‡è®ºæ–‡æ‘˜è¦ç”Ÿæˆå¤±è´¥")
                            except Exception as paper_error:
                                logger.error(f"âŒ ç¬¬ {i+1}/{total_count} ç¯‡è®ºæ–‡å¤„ç†å¼‚å¸¸: {paper_error}")

                            # æ›´æ–°è¿›åº¦
                            background_tasks['ai_analysis_progress'] = i + 1

                            # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                            import time
                            time.sleep(1)

                        logger.info(f"ğŸ‰ åå°AIæ‘˜è¦ç”Ÿæˆå®Œæˆï¼ŒæˆåŠŸå¤„ç† {analyzed_count}/{total_count} ç¯‡è®ºæ–‡")

                        # æ•°æ®åº“æ›´æ–°åï¼Œè‡ªåŠ¨æ›´æ–°æ´å¯Ÿç¼“å­˜
                        logger.info("æ•°æ®åº“å·²æ›´æ–°ï¼Œå¼€å§‹è‡ªåŠ¨æ›´æ–°æ´å¯Ÿç¼“å­˜...")

                        # æ›´æ–°ä¸åŒæ—¶é—´èŒƒå›´çš„æ´å¯Ÿ
                        for days in [1, 7, 30]:
                            try:
                                updated = global_analyzer.auto_update_insights_if_needed(days)
                                if updated:
                                    logger.info(f"æˆåŠŸæ›´æ–° {days} å¤©æ´å¯Ÿç¼“å­˜")
                                else:
                                    logger.info(f"{days} å¤©æ´å¯Ÿç¼“å­˜å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ›´æ–°")
                            except Exception as e:
                                logger.error(f"æ›´æ–° {days} å¤©æ´å¯Ÿç¼“å­˜å¤±è´¥: {e}")

                    except Exception as e:
                        logger.error(f"åå°AIæ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
                        import traceback
                        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                    finally:
                        # é‡ç½®ä»»åŠ¡çŠ¶æ€
                        background_tasks['ai_analysis_running'] = False
                        logger.info("ğŸ”„ åå°ä»»åŠ¡çŠ¶æ€å·²é‡ç½®")

                # å¯åŠ¨åå°çº¿ç¨‹
                import threading
                logger.info("ğŸš€ å‡†å¤‡å¯åŠ¨åå°çº¿ç¨‹...")
                background_thread = threading.Thread(target=background_ai_analysis, daemon=True)
                background_thread.start()

                paper_count = len(papers_to_process)
                logger.info(f"ğŸš€ å·²å¯åŠ¨åå°AIåˆ†æä»»åŠ¡ï¼Œå¤„ç† {paper_count} ç¯‡è®ºæ–‡")
                flash(f'å·²å¯åŠ¨åå°AIåˆ†æï¼Œæ­£åœ¨å¤„ç† {paper_count} ç¯‡è®ºæ–‡ï¼Œè¯·ç¨åæŸ¥çœ‹ç»“æœ', 'info')

            # è‡ªåŠ¨æ›´æ–°æ´å¯Ÿï¼ˆå¼‚æ­¥åå°æ‰§è¡Œï¼‰
            try:
                import threading
                import time

                def auto_update_insights():
                    """åå°è‡ªåŠ¨æ›´æ–°æ´å¯Ÿ"""
                    logger.info("å¼€å§‹åå°è‡ªåŠ¨æ›´æ–°ç ”ç©¶æ´å¯Ÿ...")
                    try:
                        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æ•°æ®åº“æ“ä½œå®Œæˆ
                        time.sleep(2)

                        # ç”Ÿæˆæ–°æ´å¯Ÿ
                        insights_data = analyzer.get_research_insights(7)
                        trending_data = analyzer.get_trending_topics(7)

                        # æ´å¯Ÿå·²é€šè¿‡analyzerè‡ªåŠ¨ç¼“å­˜åˆ°æ•°æ®åº“
                        logger.info("åå°æ´å¯Ÿæ›´æ–°å®Œæˆ")
                    except Exception as e:
                        logger.error(f"åå°æ´å¯Ÿæ›´æ–°å¤±è´¥: {e}")

                # å¯åŠ¨åå°çº¿ç¨‹
                insights_thread = threading.Thread(target=auto_update_insights, daemon=True)
                insights_thread.start()
                logger.info("å·²å¯åŠ¨åå°æ´å¯Ÿæ›´æ–°ä»»åŠ¡")

            except Exception as e:
                logger.error(f"å¯åŠ¨åå°æ´å¯Ÿæ›´æ–°å¤±è´¥: {e}")

            # æ´å¯Ÿç¼“å­˜ç°åœ¨é€šè¿‡æ•°æ®åº“ç®¡ç†ï¼Œæ— éœ€æ‰‹åŠ¨æ¸…é™¤
            logger.info("æ´å¯Ÿç¼“å­˜é€šè¿‡æ•°æ®åº“è‡ªåŠ¨ç®¡ç†")

    except Exception as e:
        logger.error(f"å¢é‡çˆ¬å–å¤±è´¥: {e}")
        flash(f'å¢é‡çˆ¬å–å¤±è´¥: {str(e)}', 'error')

    return redirect(url_for('index'))

@app.route('/scrape', methods=['GET'])
def scrape_get():
    """å¤„ç†GETè¯·æ±‚çš„scrapeï¼ˆé‡å®šå‘åˆ°é¦–é¡µï¼‰"""
    flash('è¯·ä½¿ç”¨è¡¨å•æäº¤æ¥æ‰§è¡Œçˆ¬å–æ“ä½œ', 'warning')
    return redirect(url_for('index'))

@app.route('/api/insights_status')
def api_insights_status():
    """è·å–æ´å¯ŸçŠ¶æ€API"""
    days = int(request.args.get('days', 7))
    cache_key = f'insights_{days}'

    cached_insights = analyzer.db.get_insights_cache(cache_key)

    status = {
        'cache_key': cache_key,
        'has_cache': cached_insights is not None,
        'last_updated': cached_insights.get('updated_at') if cached_insights else None,
        'is_generating': cache_key in analyzer._generating_insights
    }

    return jsonify(status)

@app.route('/insights')
def insights():
    """ç ”ç©¶æ´å¯Ÿ - åŸºäºæ•°æ®åº“æ›´æ–°çŠ¶æ€çš„æ™ºèƒ½ç¼“å­˜"""
    days = int(request.args.get('days', 7))

    # è·å–å½“å‰å…³é”®è¯çš„ç»„ä»¶
    current_scraper, current_analyzer, current_exporter = get_current_components()

    # ä¸ºå½“å‰å…³é”®è¯åˆ›å»ºç¼“å­˜é”®
    current_keyword = keyword_manager.get_current_keyword()
    cache_key = f'{current_keyword}_insights_{days}'

    try:
        # ç›´æ¥ä½¿ç”¨æ•°æ®åº“ç¼“å­˜ï¼Œæ— éœ€ç­‰å¾…
        logger.info(f"è·å–æ´å¯Ÿæ•°æ®: {cache_key}")

        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
        cached_data = current_analyzer.db.get_insights_cache(cache_key)

        if cached_data:
            # è·å–å½“å‰æ•°æ®çš„å“ˆå¸Œå€¼
            current_hash = current_analyzer.db.get_data_hash(days)
            logger.debug(f"ç¼“å­˜æ•°æ®å“ˆå¸Œ: {cached_data.get('data_hash', 'None')[:8]}..., å½“å‰å“ˆå¸Œ: {current_hash[:8]}...")

            if cached_data.get('data_hash') == current_hash:
                logger.info(f"æ•°æ®æœªæ›´æ–°ï¼Œä½¿ç”¨ç¼“å­˜æ´å¯Ÿ: {cache_key}")
                insights = cached_data['insights']
                trending = cached_data.get('trending', [])
            else:
                # æ•°æ®å“ˆå¸Œä¸åŒ¹é…ï¼Œå¯èƒ½éœ€è¦é‡æ–°ç”Ÿæˆ
                logger.info(f"æ•°æ®å“ˆå¸Œä¸åŒ¹é…ï¼Œå°è¯•é‡æ–°ç”Ÿæˆæ´å¯Ÿ: {cache_key}")

                # å°è¯•è·å–æœ€æ–°çš„ç¼“å­˜ï¼ˆå¯èƒ½åœ¨å…¶ä»–è¯·æ±‚ä¸­å·²ç»æ›´æ–°ï¼‰
                latest_cached_data = current_analyzer.db.get_insights_cache(cache_key)

                if latest_cached_data and latest_cached_data.get('data_hash') == current_hash:
                    logger.info(f"å‘ç°æ›´æ–°çš„ç¼“å­˜ï¼Œä½¿ç”¨æ–°æ´å¯Ÿ: {cache_key}")
                    insights = latest_cached_data['insights']
                    trending = latest_cached_data.get('trending', [])
                else:
                    # ç¡®å®éœ€è¦é‡æ–°ç”Ÿæˆ
                    logger.info(f"éœ€è¦é‡æ–°ç”Ÿæˆæ´å¯Ÿ: {cache_key}")

                    # åŒæ­¥ç”Ÿæˆæ–°æ´å¯Ÿ
                    try:
                        insights = current_analyzer.get_research_insights(days)
                        if insights and not insights.startswith("ç”Ÿæˆæ´å¯Ÿå¤±è´¥"):
                            trending = current_analyzer.get_trending_topics(days)
                        else:
                            trending = []
                    except Exception as e:
                        logger.error(f"ç”Ÿæˆæ´å¯Ÿå¤±è´¥: {e}")
                        insights = f"ç”Ÿæˆæ´å¯Ÿå¤±è´¥: {str(e)}"
                        trending = []
        else:
            # æ²¡æœ‰ç¼“å­˜æ•°æ®ï¼ŒåŒæ­¥ç”Ÿæˆï¼ˆé¦–æ¬¡è®¿é—®ï¼‰
            logger.info(f"é¦–æ¬¡è®¿é—®ï¼Œç”Ÿæˆæ´å¯Ÿ: {cache_key}")
            try:
                insights = current_analyzer.get_research_insights(days)
                if insights and not insights.startswith("ç”Ÿæˆæ´å¯Ÿå¤±è´¥"):
                    trending = current_analyzer.get_trending_topics(days)
                else:
                    trending = []
            except Exception as e:
                logger.error(f"ç”Ÿæˆæ´å¯Ÿå¤±è´¥: {e}")
                insights = f"ç”Ÿæˆæ´å¯Ÿå¤±è´¥: {str(e)}"
                trending = []

    except Exception as e:
        logger.error(f"è·å–æ´å¯Ÿå¤±è´¥: {e}")
        insights = f"è·å–æ´å¯Ÿå¤±è´¥: {str(e)}"
        trending = []

    # è®¡ç®—å®é™…çš„æ•°æ®èŒƒå›´ï¼ˆåœ¨try-exceptå—ä¹‹å¤–ï¼Œç¡®ä¿æ€»æ˜¯æ‰§è¡Œï¼‰
    try:
        actual_papers = current_analyzer.db.get_recent_papers(days)
        actual_papers_count = len(actual_papers)

        if actual_papers_count > 0:
            earliest_date = min(paper.published_date.date() for paper in actual_papers)
            latest_date = max(paper.published_date.date() for paper in actual_papers)
            actual_range = f"{earliest_date} åˆ° {latest_date}"
        else:
            actual_range = "æ— æ•°æ®"
    except Exception as e:
        logger.error(f"è·å–å®é™…æ•°æ®èŒƒå›´å¤±è´¥: {e}")
        actual_papers_count = 0
        actual_range = "æ•°æ®è·å–å¤±è´¥"

    # å°†æ´å¯Ÿå†…å®¹è½¬æ¢ä¸ºHTMLæ ¼å¼ä»¥æ”¯æŒMarkdown
    try:
        if insights and not insights.startswith("ç”Ÿæˆæ´å¯Ÿå¤±è´¥") and not insights.startswith("è·å–æ´å¯Ÿå¤±è´¥"):
            # æ¸…ç†AIç”Ÿæˆå†…å®¹ä¸­çš„markdownä»£ç å—æ ‡è®°
            cleaned_insights = insights.strip()
            if cleaned_insights.startswith('```markdown'):
                # ç§»é™¤å¼€å¤´çš„```markdown
                cleaned_insights = cleaned_insights[11:].strip()
                # ç§»é™¤ç»“å°¾çš„```
                if cleaned_insights.endswith('```'):
                    cleaned_insights = cleaned_insights[:-3].strip()
            elif cleaned_insights.startswith('```'):
                # å¤„ç†å…¶ä»–ä»£ç å—æ ‡è®°
                cleaned_insights = cleaned_insights[3:].strip()
                if cleaned_insights.endswith('```'):
                    cleaned_insights = cleaned_insights[:-3].strip()

            # ä½¿ç”¨markdownåº“è½¬æ¢å†…å®¹
            insights_html = markdown.markdown(
                cleaned_insights,
                extensions=['tables', 'fenced_code', 'toc', 'nl2br'],
                extension_configs={
                    'tables': {},
                    'fenced_code': {},
                    'toc': {},
                    'nl2br': {}
                }
            )
        else:
            insights_html = insights
    except Exception as e:
        logger.warning(f"Markdownè½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹: {e}")
        insights_html = insights

    # è·å–å½“å‰å…³é”®è¯ä¿¡æ¯ç”¨äºæ¨¡æ¿æ˜¾ç¤º
    current_config = keyword_manager.get_current_config()
    all_keywords = keyword_manager.get_all_keywords()

    return render_template('insights.html',
                     insights=insights_html,
                     trending=trending,
                     days=days,
                     actual_papers_count=actual_papers_count,
                     actual_range=actual_range,
                     current_keyword=current_config,
                     all_keywords=all_keywords)

@app.route('/compare', methods=['GET', 'POST'])
def compare():
    """è®ºæ–‡æ¯”è¾ƒ"""
    if request.method == 'POST':
        paper_ids = request.form.get('paper_ids', '').strip().split()
        paper_ids = [pid.strip() for pid in paper_ids if pid.strip()]

        if len(paper_ids) < 2:
            flash('è¯·æä¾›è‡³å°‘ä¸¤ä¸ªè®ºæ–‡ID', 'error')
        else:
            try:
                comparison = analyzer.compare_papers(paper_ids)
                return render_template('compare_result.html',
                                     comparison=comparison,
                                     paper_ids=paper_ids)
            except Exception as e:
                flash(f'æ¯”è¾ƒå¤±è´¥: {str(e)}', 'error')

    return render_template('compare.html')

@app.route('/export')
def export():
    """å¯¼å‡ºæ•°æ®"""
    export_format = request.args.get('format', 'json')
    days = int(request.args.get('days', 30))

    # è·å–å½“å‰å…³é”®è¯çš„ç»„ä»¶
    current_scraper, current_analyzer, current_exporter = get_current_components()

    # å¦‚æœdaysä¸º-1ï¼Œè·å–æ‰€æœ‰æ•°æ®
    if days == -1:
        # è·å–æ‰€æœ‰è®ºæ–‡ï¼ˆéœ€è¦ä¿®æ”¹æ•°æ®åº“æ–¹æ³•æ¥æ”¯æŒè·å–æ‰€æœ‰æ•°æ®ï¼‰
        papers = current_scraper.db.get_all_papers() if hasattr(current_scraper.db, 'get_all_papers') else current_scraper.db.get_recent_papers(3650)  # 10å¹´ä½œä¸º"å…¨éƒ¨"
    else:
        papers = current_scraper.db.get_recent_papers(days)

    if not papers:
        flash('æ²¡æœ‰æ•°æ®å¯å¯¼å‡º', 'warning')
        return redirect(url_for('papers'))

    try:
        if export_format == 'json':
            filepath = current_exporter.export_to_json(papers)
        elif export_format == 'markdown':
            filepath = current_exporter.export_to_markdown(papers)
        elif export_format == 'bibtex':
            filepath = current_exporter.export_to_bibtex(papers)
        else:
            flash('ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼', 'error')
            return redirect(url_for('papers'))

        if filepath:
            flash(f'æ•°æ®å·²å¯¼å‡ºåˆ°: {filepath}', 'success')
        else:
            flash('å¯¼å‡ºå¤±è´¥', 'error')

    except Exception as e:
        flash(f'å¯¼å‡ºå¤±è´¥: {str(e)}', 'error')

    return redirect(url_for('papers'))

@app.route('/export_page')
def export_page():
    """å¯¼å‡ºé¡µé¢"""
    return render_template('export.html')

@app.route('/api/status')
def api_status():
    """API: è·å–ç³»ç»ŸçŠ¶æ€"""
    return jsonify(scheduler.get_status())

@app.route('/api/papers')
def api_papers():
    """API: è·å–è®ºæ–‡åˆ—è¡¨"""
    # è·å–å½“å‰å…³é”®è¯çš„ç»„ä»¶
    current_scraper, current_analyzer, current_exporter = get_current_components()

    search = request.args.get('search', '').strip()
    days = int(request.args.get('days', 30))

    if search:
        papers = current_scraper.db.search_papers(search)
    else:
        papers = current_scraper.db.get_recent_papers(days)

    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    papers_data = []
    for paper in papers:
        papers_data.append({
            'title': paper.title,
            'authors': paper.authors,
            'abstract': paper.abstract[:200] + '...',
            'arxiv_id': paper.arxiv_id,
            'published_date': paper.published_date.isoformat() if paper.published_date else None,
            'categories': paper.categories,
            'summary': paper.summary[:200] + '...' if paper.summary else None
        })

    return jsonify(papers_data)

@app.route('/api/insights')
def api_insights():
    """API: è·å–ç ”ç©¶æ´å¯Ÿ"""
    # è·å–å½“å‰å…³é”®è¯çš„ç»„ä»¶
    current_scraper, current_analyzer, current_exporter = get_current_components()

    days = int(request.args.get('days', 7))
    try:
        insights = current_analyzer.get_research_insights(days)
        trending = current_analyzer.get_trending_topics(days)
        return jsonify({
            'insights': insights,
            'trending': trending
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/paper/<arxiv_id>/bibtex')
def api_paper_bibtex(arxiv_id):
    """API: è·å–å•ä¸ªè®ºæ–‡çš„BibTeXæ ¼å¼"""
    # è·å–å½“å‰å…³é”®è¯çš„ç»„ä»¶
    current_scraper, current_analyzer, current_exporter = get_current_components()

    try:
        paper = current_scraper.db.get_paper_by_arxiv_id(arxiv_id)
        if not paper:
            return jsonify({'error': 'è®ºæ–‡æœªæ‰¾åˆ°'}), 404

        # ç”ŸæˆBibTeX key
        first_author_lastname = paper.authors[0].split()[-1] if paper.authors else "Unknown"
        year = paper.published_date.year if paper.published_date else datetime.now().year
        title_words = paper.title.split()[:3]
        title_key = ''.join([word.strip('.,!?;:') for word in title_words])
        bibtex_key = f"{first_author_lastname}{year}{title_key}"

        # æ¸…ç†å¹¶æ ¼å¼åŒ–æ•°æ®
        title = paper.title.replace('{', '\\{').replace('}', '\\}').replace('&', '\\&')
        authors = ' and '.join(paper.authors)
        abstract = paper.abstract.replace('{', '\\{').replace('}', '\\}').replace('\n', ' ')

        # ç”ŸæˆBibTeXæ¡ç›®
        bibtex_entry = f"""@misc{{{bibtex_key},
  title = {{{title}}},
  author = {{{authors}}},
  year = {{{year}}},
  eprint = {{{paper.arxiv_id}}},
  archivePrefix = {{arXiv}},
  primaryClass = {{{paper.categories[0] if paper.categories else 'cs.AI'}}},"""

        if abstract:
            bibtex_entry += f"""
  abstract = {{{abstract}}},"""

        bibtex_entry += f"""
  url = {{{paper.pdf_url}}},
  howpublished = {{arXiv:{paper.arxiv_id}}}
}}"""

        return bibtex_entry

    except Exception as e:
        logger.error(f"ç”ŸæˆBibTeXå¤±è´¥: {e}")
        return jsonify({'error': f'ç”ŸæˆBibTeXå¤±è´¥: {str(e)}'}), 500

if __name__ == '__main__':
    # åˆ›å»ºæ¨¡æ¿ç›®å½•
    import os
    os.makedirs('templates', exist_ok=True)
    os.makedirs('static', exist_ok=True)

    print("ğŸŒ å¯åŠ¨Webç•Œé¢...")
    print("ğŸ“± è®¿é—®åœ°å€: http://localhost:5000")
    print("ğŸ’¡ ä½¿ç”¨ Ctrl+C åœæ­¢æœåŠ¡")

    app.run(host='0.0.0.0', port=5000, debug=True)